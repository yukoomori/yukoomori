{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "import lightgbm as lgb\n",
    "# 評価指標\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from tqdm import tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "submission_df = pd.read_csv(\"sample_submit.csv\", names=['id', 'pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop(['id'], axis=1)\n",
    "test_df = test_df.drop(['id'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 関節ビル(B_Bil)　＋　直接ビル(D_Bil)　＝　総ビル(T_Bil)\n",
    "train_df[\"B_Bil\"] = train_df[\"T_Bil\"].values - train_df[\"D_Bil\"].values\n",
    "test_df[\"B_Bil\"] = test_df[\"T_Bil\"].values - test_df[\"D_Bil\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if abs(AST / ALT) > 0 then bad\n",
    "train_df[\"AST_ALT_ratio\"] = abs(train_df[\"AST_GOT\"].values/train_df[\"ALT_GPT\"].values)\n",
    "test_df[\"AST_ALT_ratio\"] = abs(test_df[\"AST_GOT\"].values/test_df[\"ALT_GPT\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gender = pd.get_dummies(train_df['Gender'])\n",
    "train_df = pd.concat([train_df, df_gender], axis=1)\n",
    "train_df = train_df.drop(\"Gender\", axis=1)\n",
    "\n",
    "df_gender = pd.get_dummies(test_df['Gender'])\n",
    "test_df = pd.concat([test_df, df_gender], axis=1)\n",
    "test_df = test_df.drop(\"Gender\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = list(train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns.remove(\"disease\")\n",
    "columns.remove(\"Female\")\n",
    "columns.remove(\"Male\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/pandas/core/series.py:726: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "for col in columns:\n",
    "    col_name = str(col) + \"_log\"\n",
    "    train_df[col_name] = np.log(train_df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns2 = list(test_df.columns)\n",
    "\n",
    "columns2.remove(\"Female\")\n",
    "columns2.remove(\"Male\")\n",
    "\n",
    "for col in columns2:\n",
    "    col_name = str(col) + \"_log\"\n",
    "    test_df[col_name] = np.log(test_df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['B_Bil_log'] = train_df['B_Bil_log'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data (Normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = MinMaxScaler()\n",
    "train_df2 = s.fit_transform(train_df)\n",
    "test_df2 = s.fit_transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df2 = pd.DataFrame(train_df2, columns = train_df.columns)\n",
    "test_df2 = pd.DataFrame(test_df2, columns = test_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Pearson Correlation\n",
    "# plt.figure(figsize=(12,10))\n",
    "# cor = train_df2.corr()\n",
    "# sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train_df2\n",
    "test_df = test_df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[[\"disease\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.astype({\"disease\": int}).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop([\"disease\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best features = Age Gender T_Bil ALP ALT_GPT TP Alb AG_ratio B_Bil AST_ALT_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Age', 'T_Bil', 'D_Bil', 'ALP', 'ALT_GPT', 'AST_GOT', 'TP', 'Alb',\n",
       "       'AG_ratio', 'B_Bil', 'AST_ALT_ratio', 'Female', 'Male', 'Age_log',\n",
       "       'T_Bil_log', 'D_Bil_log', 'ALP_log', 'ALT_GPT_log', 'AST_GOT_log',\n",
       "       'TP_log', 'Alb_log', 'AG_ratio_log', 'B_Bil_log', 'AST_ALT_ratio_log'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_best = X[[\"Age\", \"Gender\", \"T_Bil\", \"ALP\" ,\"ALT_GPT\" ,\"TP\", \"Alb\", \"AG_ratio\" ,\"B_Bil\" ,\"AST_ALT_ratio\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Training the Model (Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7622857142857142\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "logReg_clf = LogisticRegression()\n",
    "logReg_model = Pipeline(steps = [(\"classifier\", logReg_clf)])\n",
    "\n",
    "logReg_model.fit(X_train, y_train)\n",
    "\n",
    "score = logReg_model.score(X_val, y_val)\n",
    "\n",
    "print(\"Accuracy:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Training Model (Naives Bayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7348571428571429\n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple Naive Bayes on Counts\n",
    "naiveBayes_model = MultinomialNB()\n",
    "\n",
    "naiveBayes_model.fit(X_train, y_train)\n",
    "\n",
    "score = naiveBayes_model.score(X_val, y_val)\n",
    "\n",
    "print(\"Accuracy:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. Training Model (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8537142857142858\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_clf = RandomForestClassifier(n_estimators=200,\n",
    "                                random_state = seed)\n",
    "rf_model = Pipeline(steps = [(\"classifier\", rf_clf)])\n",
    "\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "score = rf_model.score(X_val, y_val)\n",
    "\n",
    "print(\"Accuracy:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E. Traning Model (Linear Support Vector Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7702857142857142\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svc_clf = LinearSVC(max_iter = 1000,\n",
    "                    random_state = seed)\n",
    "\n",
    "svc_clf.fit(X_train, y_train)\n",
    "\n",
    "score = svc_clf.score(X_val, y_val)\n",
    "\n",
    "print(\"Accuracy:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F. Training Model (Extra Trees Classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8537142857142858\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "et_clf = ExtraTreesClassifier(n_estimators = 500,\n",
    "                              random_state = seed)\n",
    "\n",
    "et_clf.fit(X_train, y_train)\n",
    "\n",
    "score = et_clf.score(X_val, y_val)\n",
    "\n",
    "print(\"Accuracy:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E. LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              R2       MAE       MSE      RMSE\n",
      "scores  0.546977  0.208255  0.110851  0.332943\n"
     ]
    }
   ],
   "source": [
    "scores = calculate_scores(y_test, y_pred)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              R2       MAE       MSE      RMSE\n",
      "scores  0.549591  0.210937  0.110211  0.331981\n"
     ]
    }
   ],
   "source": [
    "scores = calculate_scores(y_test, y_pred)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              R2       MAE       MSE      RMSE\n",
      "scores  0.549591  0.210937  0.110211  0.331981\n"
     ]
    }
   ],
   "source": [
    "scores = calculate_scores(y_test, y_pred)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              R2       MAE       MSE      RMSE\n",
      "scores  0.551724  0.208123  0.109689  0.331194\n"
     ]
    }
   ],
   "source": [
    "scores = calculate_scores(y_test, y_pred)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              R2       MAE       MSE      RMSE\n",
      "scores  0.551724  0.208123  0.109689  0.331194\n"
     ]
    }
   ],
   "source": [
    "scores = calculate_scores(y_test, y_pred)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              R2       MAE       MSE      RMSE\n",
      "scores  0.549591  0.210937  0.110211  0.331981\n"
     ]
    }
   ],
   "source": [
    "scores = calculate_scores(y_test, y_pred)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習データと評価データを作成\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "# trainのデータセットの3割をモデル学習時のバリデーションデータとして利用する\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.2, random_state=seed)\n",
    "\n",
    "# LightGBMを利用するのに必要なフォーマットに変換\n",
    "lgb_train = lgb.Dataset(x_train, y_train)\n",
    "lgb_eval = lgb.Dataset(x_valid, y_valid, reference=lgb_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': {'l2','l1'},\n",
    "    'num_leaves': 50,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'vervose': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: vervose\n",
      "[LightGBM] [Warning] Unknown parameter: vervose\n",
      "[LightGBM] [Warning] Unknown parameter: vervose\n",
      "[LightGBM] [Warning] Unknown parameter: vervose\n",
      "[LightGBM] [Warning] Unknown parameter: vervose\n",
      "[LightGBM] [Info] Number of positive: 915, number of negative: 1325\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000661 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5216\n",
      "[LightGBM] [Info] Number of data points in the train set: 2240, number of used features: 24\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.408482 -> initscore=-0.370244\n",
      "[LightGBM] [Info] Start training from score -0.370244\n",
      "[1]\tvalid_0's l2: 0.232402\tvalid_0's l1: 0.473611\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[2]\tvalid_0's l2: 0.222573\tvalid_0's l1: 0.462941\n",
      "[3]\tvalid_0's l2: 0.214291\tvalid_0's l1: 0.453167\n",
      "[4]\tvalid_0's l2: 0.206359\tvalid_0's l1: 0.443446\n",
      "[5]\tvalid_0's l2: 0.199467\tvalid_0's l1: 0.434599\n",
      "[6]\tvalid_0's l2: 0.192723\tvalid_0's l1: 0.425842\n",
      "[7]\tvalid_0's l2: 0.186488\tvalid_0's l1: 0.417296\n",
      "[8]\tvalid_0's l2: 0.180673\tvalid_0's l1: 0.408891\n",
      "[9]\tvalid_0's l2: 0.175263\tvalid_0's l1: 0.400758\n",
      "[10]\tvalid_0's l2: 0.170521\tvalid_0's l1: 0.393205\n",
      "[11]\tvalid_0's l2: 0.165997\tvalid_0's l1: 0.385776\n",
      "[12]\tvalid_0's l2: 0.161645\tvalid_0's l1: 0.378648\n",
      "[13]\tvalid_0's l2: 0.157812\tvalid_0's l1: 0.371858\n",
      "[14]\tvalid_0's l2: 0.154994\tvalid_0's l1: 0.366092\n",
      "[15]\tvalid_0's l2: 0.151632\tvalid_0's l1: 0.359648\n",
      "[16]\tvalid_0's l2: 0.148357\tvalid_0's l1: 0.353506\n",
      "[17]\tvalid_0's l2: 0.145319\tvalid_0's l1: 0.3475\n",
      "[18]\tvalid_0's l2: 0.142673\tvalid_0's l1: 0.34183\n",
      "[19]\tvalid_0's l2: 0.140178\tvalid_0's l1: 0.336314\n",
      "[20]\tvalid_0's l2: 0.137889\tvalid_0's l1: 0.330923\n",
      "[21]\tvalid_0's l2: 0.13628\tvalid_0's l1: 0.326659\n",
      "[22]\tvalid_0's l2: 0.13529\tvalid_0's l1: 0.323134\n",
      "[23]\tvalid_0's l2: 0.134111\tvalid_0's l1: 0.319261\n",
      "[24]\tvalid_0's l2: 0.133532\tvalid_0's l1: 0.316008\n",
      "[25]\tvalid_0's l2: 0.132408\tvalid_0's l1: 0.312104\n",
      "[26]\tvalid_0's l2: 0.131141\tvalid_0's l1: 0.308194\n",
      "[27]\tvalid_0's l2: 0.130224\tvalid_0's l1: 0.304949\n",
      "[28]\tvalid_0's l2: 0.129567\tvalid_0's l1: 0.301703\n",
      "[29]\tvalid_0's l2: 0.12885\tvalid_0's l1: 0.298268\n",
      "[30]\tvalid_0's l2: 0.128359\tvalid_0's l1: 0.2953\n",
      "[31]\tvalid_0's l2: 0.127218\tvalid_0's l1: 0.291658\n",
      "[32]\tvalid_0's l2: 0.126095\tvalid_0's l1: 0.288052\n",
      "[33]\tvalid_0's l2: 0.125361\tvalid_0's l1: 0.285109\n",
      "[34]\tvalid_0's l2: 0.124635\tvalid_0's l1: 0.282129\n",
      "[35]\tvalid_0's l2: 0.124099\tvalid_0's l1: 0.279503\n",
      "[36]\tvalid_0's l2: 0.123499\tvalid_0's l1: 0.276771\n",
      "[37]\tvalid_0's l2: 0.122891\tvalid_0's l1: 0.274169\n",
      "[38]\tvalid_0's l2: 0.122355\tvalid_0's l1: 0.271675\n",
      "[39]\tvalid_0's l2: 0.122031\tvalid_0's l1: 0.269434\n",
      "[40]\tvalid_0's l2: 0.12165\tvalid_0's l1: 0.267325\n",
      "[41]\tvalid_0's l2: 0.121152\tvalid_0's l1: 0.265292\n",
      "[42]\tvalid_0's l2: 0.120513\tvalid_0's l1: 0.263052\n",
      "[43]\tvalid_0's l2: 0.120168\tvalid_0's l1: 0.261248\n",
      "[44]\tvalid_0's l2: 0.119986\tvalid_0's l1: 0.259516\n",
      "[45]\tvalid_0's l2: 0.119366\tvalid_0's l1: 0.257605\n",
      "[46]\tvalid_0's l2: 0.119064\tvalid_0's l1: 0.255808\n",
      "[47]\tvalid_0's l2: 0.118771\tvalid_0's l1: 0.253888\n",
      "[48]\tvalid_0's l2: 0.119234\tvalid_0's l1: 0.252738\n",
      "[49]\tvalid_0's l2: 0.119567\tvalid_0's l1: 0.251519\n",
      "[50]\tvalid_0's l2: 0.119757\tvalid_0's l1: 0.25019\n",
      "[51]\tvalid_0's l2: 0.119754\tvalid_0's l1: 0.248746\n",
      "[52]\tvalid_0's l2: 0.11968\tvalid_0's l1: 0.247132\n",
      "[53]\tvalid_0's l2: 0.119522\tvalid_0's l1: 0.245765\n",
      "[54]\tvalid_0's l2: 0.119367\tvalid_0's l1: 0.244054\n",
      "[55]\tvalid_0's l2: 0.119282\tvalid_0's l1: 0.242545\n",
      "[56]\tvalid_0's l2: 0.119372\tvalid_0's l1: 0.241348\n",
      "[57]\tvalid_0's l2: 0.119257\tvalid_0's l1: 0.239983\n",
      "[58]\tvalid_0's l2: 0.119217\tvalid_0's l1: 0.238929\n",
      "[59]\tvalid_0's l2: 0.119196\tvalid_0's l1: 0.238034\n",
      "[60]\tvalid_0's l2: 0.11892\tvalid_0's l1: 0.236987\n",
      "[61]\tvalid_0's l2: 0.11898\tvalid_0's l1: 0.236225\n",
      "[62]\tvalid_0's l2: 0.119023\tvalid_0's l1: 0.235327\n",
      "[63]\tvalid_0's l2: 0.119161\tvalid_0's l1: 0.234478\n",
      "[64]\tvalid_0's l2: 0.11948\tvalid_0's l1: 0.233832\n",
      "[65]\tvalid_0's l2: 0.119643\tvalid_0's l1: 0.233055\n",
      "[66]\tvalid_0's l2: 0.119626\tvalid_0's l1: 0.232076\n",
      "[67]\tvalid_0's l2: 0.119618\tvalid_0's l1: 0.231229\n",
      "[68]\tvalid_0's l2: 0.119339\tvalid_0's l1: 0.230149\n",
      "[69]\tvalid_0's l2: 0.119156\tvalid_0's l1: 0.229142\n",
      "[70]\tvalid_0's l2: 0.119164\tvalid_0's l1: 0.228539\n",
      "[71]\tvalid_0's l2: 0.119145\tvalid_0's l1: 0.227828\n",
      "[72]\tvalid_0's l2: 0.119147\tvalid_0's l1: 0.227155\n",
      "[73]\tvalid_0's l2: 0.118999\tvalid_0's l1: 0.22656\n",
      "[74]\tvalid_0's l2: 0.119031\tvalid_0's l1: 0.225981\n",
      "[75]\tvalid_0's l2: 0.119107\tvalid_0's l1: 0.225408\n",
      "[76]\tvalid_0's l2: 0.119003\tvalid_0's l1: 0.224492\n",
      "[77]\tvalid_0's l2: 0.119074\tvalid_0's l1: 0.223738\n",
      "[78]\tvalid_0's l2: 0.119137\tvalid_0's l1: 0.223033\n",
      "[79]\tvalid_0's l2: 0.119053\tvalid_0's l1: 0.222306\n",
      "[80]\tvalid_0's l2: 0.118891\tvalid_0's l1: 0.221497\n",
      "[81]\tvalid_0's l2: 0.118705\tvalid_0's l1: 0.220635\n",
      "[82]\tvalid_0's l2: 0.118968\tvalid_0's l1: 0.22033\n",
      "[83]\tvalid_0's l2: 0.118951\tvalid_0's l1: 0.219758\n",
      "[84]\tvalid_0's l2: 0.118671\tvalid_0's l1: 0.218991\n",
      "[85]\tvalid_0's l2: 0.118465\tvalid_0's l1: 0.218404\n",
      "[86]\tvalid_0's l2: 0.118407\tvalid_0's l1: 0.217808\n",
      "[87]\tvalid_0's l2: 0.118458\tvalid_0's l1: 0.217376\n",
      "[88]\tvalid_0's l2: 0.118771\tvalid_0's l1: 0.217379\n",
      "[89]\tvalid_0's l2: 0.118652\tvalid_0's l1: 0.216832\n",
      "[90]\tvalid_0's l2: 0.11867\tvalid_0's l1: 0.216345\n",
      "[91]\tvalid_0's l2: 0.119068\tvalid_0's l1: 0.216271\n",
      "[92]\tvalid_0's l2: 0.119211\tvalid_0's l1: 0.215865\n",
      "[93]\tvalid_0's l2: 0.119371\tvalid_0's l1: 0.215697\n",
      "[94]\tvalid_0's l2: 0.119567\tvalid_0's l1: 0.215578\n",
      "[95]\tvalid_0's l2: 0.119813\tvalid_0's l1: 0.215432\n",
      "[96]\tvalid_0's l2: 0.12006\tvalid_0's l1: 0.214976\n",
      "[97]\tvalid_0's l2: 0.12033\tvalid_0's l1: 0.21481\n",
      "[98]\tvalid_0's l2: 0.120423\tvalid_0's l1: 0.214458\n",
      "[99]\tvalid_0's l2: 0.120413\tvalid_0's l1: 0.214039\n",
      "[100]\tvalid_0's l2: 0.12098\tvalid_0's l1: 0.21393\n",
      "[101]\tvalid_0's l2: 0.121176\tvalid_0's l1: 0.2137\n",
      "[102]\tvalid_0's l2: 0.121222\tvalid_0's l1: 0.213403\n",
      "[103]\tvalid_0's l2: 0.121335\tvalid_0's l1: 0.21319\n",
      "[104]\tvalid_0's l2: 0.121087\tvalid_0's l1: 0.212733\n",
      "[105]\tvalid_0's l2: 0.121415\tvalid_0's l1: 0.212552\n",
      "[106]\tvalid_0's l2: 0.121454\tvalid_0's l1: 0.212168\n",
      "[107]\tvalid_0's l2: 0.121403\tvalid_0's l1: 0.21169\n",
      "[108]\tvalid_0's l2: 0.121347\tvalid_0's l1: 0.211268\n",
      "[109]\tvalid_0's l2: 0.121085\tvalid_0's l1: 0.210689\n",
      "[110]\tvalid_0's l2: 0.121148\tvalid_0's l1: 0.210503\n",
      "[111]\tvalid_0's l2: 0.121281\tvalid_0's l1: 0.210321\n",
      "[112]\tvalid_0's l2: 0.121306\tvalid_0's l1: 0.209913\n",
      "[113]\tvalid_0's l2: 0.121594\tvalid_0's l1: 0.209797\n",
      "[114]\tvalid_0's l2: 0.121613\tvalid_0's l1: 0.209427\n",
      "[115]\tvalid_0's l2: 0.121728\tvalid_0's l1: 0.209049\n",
      "[116]\tvalid_0's l2: 0.12152\tvalid_0's l1: 0.20861\n",
      "[117]\tvalid_0's l2: 0.12123\tvalid_0's l1: 0.208016\n",
      "[118]\tvalid_0's l2: 0.121204\tvalid_0's l1: 0.207683\n",
      "[119]\tvalid_0's l2: 0.121253\tvalid_0's l1: 0.207391\n",
      "[120]\tvalid_0's l2: 0.121303\tvalid_0's l1: 0.207078\n",
      "[121]\tvalid_0's l2: 0.121349\tvalid_0's l1: 0.206818\n",
      "[122]\tvalid_0's l2: 0.121378\tvalid_0's l1: 0.206648\n",
      "[123]\tvalid_0's l2: 0.12149\tvalid_0's l1: 0.20638\n",
      "[124]\tvalid_0's l2: 0.121543\tvalid_0's l1: 0.206041\n",
      "[125]\tvalid_0's l2: 0.121705\tvalid_0's l1: 0.206141\n",
      "[126]\tvalid_0's l2: 0.121479\tvalid_0's l1: 0.205535\n",
      "[127]\tvalid_0's l2: 0.121552\tvalid_0's l1: 0.205378\n",
      "[128]\tvalid_0's l2: 0.121717\tvalid_0's l1: 0.205197\n",
      "[129]\tvalid_0's l2: 0.121799\tvalid_0's l1: 0.204904\n",
      "[130]\tvalid_0's l2: 0.121709\tvalid_0's l1: 0.204394\n",
      "[131]\tvalid_0's l2: 0.121597\tvalid_0's l1: 0.204015\n",
      "[132]\tvalid_0's l2: 0.121433\tvalid_0's l1: 0.203581\n",
      "[133]\tvalid_0's l2: 0.121196\tvalid_0's l1: 0.203074\n",
      "[134]\tvalid_0's l2: 0.12114\tvalid_0's l1: 0.202581\n",
      "[135]\tvalid_0's l2: 0.121148\tvalid_0's l1: 0.202259\n",
      "[136]\tvalid_0's l2: 0.120959\tvalid_0's l1: 0.201806\n",
      "Early stopping, best iteration is:\n",
      "[86]\tvalid_0's l2: 0.118407\tvalid_0's l1: 0.217808\n"
     ]
    }
   ],
   "source": [
    "# LightGBM学習\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=1000,\n",
    "                valid_sets=lgb_eval,\n",
    "                early_stopping_rounds=50\n",
    "               )\n",
    "\n",
    "# LightGBM推論\n",
    "y_pred = gbm.predict(x_test, num_iteration=gbm.best_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 評価\n",
    "def calculate_scores(true, pred):\n",
    "    \"\"\"全ての評価指標を計算する\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    true (np.array)       : 実測値\n",
    "    pred (np.array)       : 予測値\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    scores (pd.DataFrame) : 各評価指標を纏めた結果\n",
    "\n",
    "    \"\"\"\n",
    "    scores = {}\n",
    "    scores = pd.DataFrame({'R2': r2_score(true, pred),\n",
    "                          'MAE': mean_absolute_error(true, pred),\n",
    "                          'MSE': mean_squared_error(true, pred),\n",
    "                          'RMSE': np.sqrt(mean_squared_error(true, pred))},\n",
    "                           index = ['scores'])\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALL features, objective = regression: \n",
    "#               R2       MAE       MSE     RMSE\n",
    "# scores  0.530204  0.229578  0.114955  0.33905"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              R2       MAE       MSE      RMSE\n",
      "scores  0.546977  0.208255  0.110851  0.332943\n"
     ]
    }
   ],
   "source": [
    "# ALL features, objective = binary: \n",
    "#               R2       MAE       MSE      RMSE\n",
    "# scores  0.546977  0.208255  0.110851  0.332943\n",
    "\n",
    "scores = calculate_scores(y_test, y_pred)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 features, objective = regression: \n",
    "#               R2       MAE       MSE      RMSE\n",
    "# scores  0.519066  0.231667  0.117681  0.343046"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 features, objective = binary: \n",
    "#               R2       MAE       MSE      RMSE\n",
    "# scores  0.534537  0.213649  0.113895  0.337483"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "LightGBMError",
     "evalue": "The number of features in data (23) is not the same as it was in training data (24).\nYou can set ``predict_disable_shape_check=true`` to discard this error, but please be aware what you are doing.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLightGBMError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-169-e1c60b26de3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred_gbm_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgbm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, start_iteration, num_iteration, raw_score, pred_leaf, pred_contrib, data_has_header, is_reshape, **kwargs)\u001b[0m\n\u001b[1;32m   3140\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3141\u001b[0m                 \u001b[0mnum_iteration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3142\u001b[0;31m         return predictor.predict(data, start_iteration, num_iteration,\n\u001b[0m\u001b[1;32m   3143\u001b[0m                                  \u001b[0mraw_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_leaf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_contrib\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3144\u001b[0m                                  data_has_header, is_reshape)\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, start_iteration, num_iteration, raw_score, pred_leaf, pred_contrib, data_has_header, is_reshape)\u001b[0m\n\u001b[1;32m    726\u001b[0m             \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__pred_for_csc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_iteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m             \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__pred_for_np2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_iteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    729\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36m__pred_for_np2d\u001b[0;34m(self, mat, start_iteration, num_iteration, predict_type)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0minner_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_iteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     def __create_sparse_native(self, cs, out_shape, out_ptr_indptr, out_ptr_indices, out_ptr_data,\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36minner_predict\u001b[0;34m(mat, start_iteration, num_iteration, predict_type, preds)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Wrong length of pre-allocated predict array\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m             \u001b[0mout_num_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m             _safe_call(_LIB.LGBM_BoosterPredictForMat(\n\u001b[0m\u001b[1;32m    790\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m                 \u001b[0mptr_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36m_safe_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \"\"\"\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mLightGBMError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLGBM_GetLastError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLightGBMError\u001b[0m: The number of features in data (23) is not the same as it was in training data (24).\nYou can set ``predict_disable_shape_check=true`` to discard this error, but please be aware what you are doing."
     ]
    }
   ],
   "source": [
    "y_pred_gbm_val = gbm.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_gbm_val_int = np.round(y_pred_gbm_val).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "466 51\n",
      "70 288\n",
      "correct: 754 incorrect: 121\n"
     ]
    }
   ],
   "source": [
    "a,b,c,d = confusion_matrix(y_val, y_pred_gbm_val_int).ravel()\n",
    "print(a, b)\n",
    "print(c, d)\n",
    "print(\"correct:\",str(a+d), \"incorrect:\",str(b+c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass n_features_to_select=1 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Features: 1\n",
      "Selected Features: [False False False False False False False False False False False False\n",
      " False False  True False False False False False False False False False]\n",
      "Feature Ranking: [10 24 22 21  9 15 19  7 16 23 17 18 13 11  1  3  4  2  5  8 12 20  6 14]\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(solver='lbfgs')\n",
    "rfe = RFE(model, 1)\n",
    "fit = rfe.fit(X_train, y_train)\n",
    "print(\"Num Features: %d\" % fit.n_features_)\n",
    "print(\"Selected Features: %s\" % fit.support_)\n",
    "print(\"Feature Ranking: %s\" % fit.ranking_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking = list(fit.ranking_)\n",
    "col = X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>T_Bil_log</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ALT_GPT_log</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>D_Bil_log</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ALP_log</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>AST_GOT_log</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>B_Bil_log</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Alb</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>TP_log</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ALT_GPT</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Age</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Age_log</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Alb_log</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Male</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>AST_ALT_ratio_log</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AST_GOT</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AG_ratio</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>AST_ALT_ratio</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Female</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TP</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>AG_ratio_log</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ALP</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D_Bil</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>B_Bil</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T_Bil</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              feature  importance\n",
       "14          T_Bil_log           1\n",
       "17        ALT_GPT_log           2\n",
       "15          D_Bil_log           3\n",
       "16            ALP_log           4\n",
       "18        AST_GOT_log           5\n",
       "22          B_Bil_log           6\n",
       "7                 Alb           7\n",
       "19             TP_log           8\n",
       "4             ALT_GPT           9\n",
       "0                 Age          10\n",
       "13            Age_log          11\n",
       "20            Alb_log          12\n",
       "12               Male          13\n",
       "23  AST_ALT_ratio_log          14\n",
       "5             AST_GOT          15\n",
       "8            AG_ratio          16\n",
       "10      AST_ALT_ratio          17\n",
       "11             Female          18\n",
       "6                  TP          19\n",
       "21       AG_ratio_log          20\n",
       "3                 ALP          21\n",
       "2               D_Bil          22\n",
       "9               B_Bil          23\n",
       "1               T_Bil          24"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_importance = pd.DataFrame({'feature': col,'importance': ranking,})\n",
    "df_importance = df_importance.sort_values(by = \"importance\")\n",
    "df_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T_Bil_log',\n",
       " 'ALT_GPT_log',\n",
       " 'D_Bil_log',\n",
       " 'ALP_log',\n",
       " 'AST_GOT_log',\n",
       " 'B_Bil_log',\n",
       " 'Alb',\n",
       " 'TP_log',\n",
       " 'ALT_GPT',\n",
       " 'Age',\n",
       " 'Age_log',\n",
       " 'Alb_log',\n",
       " 'Male',\n",
       " 'AST_ALT_ratio_log',\n",
       " 'AST_GOT',\n",
       " 'AG_ratio',\n",
       " 'AST_ALT_ratio',\n",
       " 'Female',\n",
       " 'TP',\n",
       " 'AG_ratio_log',\n",
       " 'ALP',\n",
       " 'D_Bil',\n",
       " 'B_Bil']"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top = df_importance[\"feature\"].to_list()[:-1]\n",
    "top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_imp = X[top]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T_Bil_log</th>\n",
       "      <th>ALT_GPT_log</th>\n",
       "      <th>D_Bil_log</th>\n",
       "      <th>ALP_log</th>\n",
       "      <th>AST_GOT_log</th>\n",
       "      <th>B_Bil_log</th>\n",
       "      <th>Alb</th>\n",
       "      <th>TP_log</th>\n",
       "      <th>ALT_GPT</th>\n",
       "      <th>Age</th>\n",
       "      <th>...</th>\n",
       "      <th>AST_ALT_ratio_log</th>\n",
       "      <th>AST_GOT</th>\n",
       "      <th>AG_ratio</th>\n",
       "      <th>AST_ALT_ratio</th>\n",
       "      <th>Female</th>\n",
       "      <th>TP</th>\n",
       "      <th>AG_ratio_log</th>\n",
       "      <th>ALP</th>\n",
       "      <th>D_Bil</th>\n",
       "      <th>B_Bil</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.089946</td>\n",
       "      <td>0.237739</td>\n",
       "      <td>0.223862</td>\n",
       "      <td>0.085380</td>\n",
       "      <td>0.316059</td>\n",
       "      <td>0.426557</td>\n",
       "      <td>0.603580</td>\n",
       "      <td>0.679707</td>\n",
       "      <td>0.011540</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.565627</td>\n",
       "      <td>0.047381</td>\n",
       "      <td>0.187176</td>\n",
       "      <td>0.080215</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.578569</td>\n",
       "      <td>0.390896</td>\n",
       "      <td>0.019046</td>\n",
       "      <td>0.007005</td>\n",
       "      <td>0.395024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.135594</td>\n",
       "      <td>0.128030</td>\n",
       "      <td>0.244362</td>\n",
       "      <td>0.114341</td>\n",
       "      <td>0.023488</td>\n",
       "      <td>0.456205</td>\n",
       "      <td>0.573186</td>\n",
       "      <td>0.717510</td>\n",
       "      <td>0.004393</td>\n",
       "      <td>0.861111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.465377</td>\n",
       "      <td>0.001837</td>\n",
       "      <td>0.257358</td>\n",
       "      <td>0.043760</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.622254</td>\n",
       "      <td>0.483838</td>\n",
       "      <td>0.026558</td>\n",
       "      <td>0.008247</td>\n",
       "      <td>0.398277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.202945</td>\n",
       "      <td>0.113081</td>\n",
       "      <td>0.371615</td>\n",
       "      <td>0.058837</td>\n",
       "      <td>0.012537</td>\n",
       "      <td>0.472039</td>\n",
       "      <td>0.573095</td>\n",
       "      <td>0.804082</td>\n",
       "      <td>0.003709</td>\n",
       "      <td>0.361111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.471926</td>\n",
       "      <td>0.000959</td>\n",
       "      <td>0.305783</td>\n",
       "      <td>0.045557</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.728047</td>\n",
       "      <td>0.539257</td>\n",
       "      <td>0.012654</td>\n",
       "      <td>0.020544</td>\n",
       "      <td>0.400286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.121448</td>\n",
       "      <td>0.140774</td>\n",
       "      <td>0.185741</td>\n",
       "      <td>0.118829</td>\n",
       "      <td>0.091682</td>\n",
       "      <td>0.456470</td>\n",
       "      <td>0.552490</td>\n",
       "      <td>0.732406</td>\n",
       "      <td>0.005021</td>\n",
       "      <td>0.861111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500932</td>\n",
       "      <td>0.008262</td>\n",
       "      <td>0.257125</td>\n",
       "      <td>0.054376</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.639878</td>\n",
       "      <td>0.483555</td>\n",
       "      <td>0.027774</td>\n",
       "      <td>0.005066</td>\n",
       "      <td>0.398309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.328243</td>\n",
       "      <td>0.216428</td>\n",
       "      <td>0.451248</td>\n",
       "      <td>0.132027</td>\n",
       "      <td>0.408537</td>\n",
       "      <td>0.548603</td>\n",
       "      <td>0.240197</td>\n",
       "      <td>0.695803</td>\n",
       "      <td>0.009799</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>...</td>\n",
       "      <td>0.650512</td>\n",
       "      <td>0.076940</td>\n",
       "      <td>0.259223</td>\n",
       "      <td>0.132447</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.596989</td>\n",
       "      <td>0.486088</td>\n",
       "      <td>0.031439</td>\n",
       "      <td>0.034659</td>\n",
       "      <td>0.413492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3495</th>\n",
       "      <td>0.150118</td>\n",
       "      <td>0.198362</td>\n",
       "      <td>0.250888</td>\n",
       "      <td>0.100623</td>\n",
       "      <td>0.120821</td>\n",
       "      <td>0.465609</td>\n",
       "      <td>0.815802</td>\n",
       "      <td>0.941273</td>\n",
       "      <td>0.008474</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.466581</td>\n",
       "      <td>0.011589</td>\n",
       "      <td>0.286878</td>\n",
       "      <td>0.044085</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.913421</td>\n",
       "      <td>0.518339</td>\n",
       "      <td>0.022927</td>\n",
       "      <td>0.008675</td>\n",
       "      <td>0.399445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3496</th>\n",
       "      <td>0.580763</td>\n",
       "      <td>0.247009</td>\n",
       "      <td>0.662246</td>\n",
       "      <td>0.522152</td>\n",
       "      <td>0.322759</td>\n",
       "      <td>0.677669</td>\n",
       "      <td>0.224272</td>\n",
       "      <td>0.721254</td>\n",
       "      <td>0.012362</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.561502</td>\n",
       "      <td>0.049175</td>\n",
       "      <td>0.048812</td>\n",
       "      <td>0.078267</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.626662</td>\n",
       "      <td>0.134320</td>\n",
       "      <td>0.225528</td>\n",
       "      <td>0.129379</td>\n",
       "      <td>0.457451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3497</th>\n",
       "      <td>0.073858</td>\n",
       "      <td>0.216871</td>\n",
       "      <td>0.229708</td>\n",
       "      <td>0.080811</td>\n",
       "      <td>0.185622</td>\n",
       "      <td>0.412876</td>\n",
       "      <td>0.099244</td>\n",
       "      <td>0.506385</td>\n",
       "      <td>0.009833</td>\n",
       "      <td>0.861111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.494287</td>\n",
       "      <td>0.020537</td>\n",
       "      <td>0.017626</td>\n",
       "      <td>0.052225</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.396156</td>\n",
       "      <td>0.052715</td>\n",
       "      <td>0.017913</td>\n",
       "      <td>0.007344</td>\n",
       "      <td>0.393720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3498</th>\n",
       "      <td>0.336230</td>\n",
       "      <td>0.163243</td>\n",
       "      <td>0.291304</td>\n",
       "      <td>0.302112</td>\n",
       "      <td>0.366636</td>\n",
       "      <td>0.591029</td>\n",
       "      <td>0.053591</td>\n",
       "      <td>0.194500</td>\n",
       "      <td>0.006241</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>...</td>\n",
       "      <td>0.671759</td>\n",
       "      <td>0.062188</td>\n",
       "      <td>0.011676</td>\n",
       "      <td>0.149998</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.131381</td>\n",
       "      <td>0.035529</td>\n",
       "      <td>0.092271</td>\n",
       "      <td>0.011737</td>\n",
       "      <td>0.424150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3499</th>\n",
       "      <td>0.308792</td>\n",
       "      <td>0.259199</td>\n",
       "      <td>0.393897</td>\n",
       "      <td>0.310660</td>\n",
       "      <td>0.290208</td>\n",
       "      <td>0.551986</td>\n",
       "      <td>0.853258</td>\n",
       "      <td>0.674075</td>\n",
       "      <td>0.013508</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.527168</td>\n",
       "      <td>0.040894</td>\n",
       "      <td>0.268835</td>\n",
       "      <td>0.063718</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.572186</td>\n",
       "      <td>0.497532</td>\n",
       "      <td>0.096119</td>\n",
       "      <td>0.023840</td>\n",
       "      <td>0.414239</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3500 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      T_Bil_log  ALT_GPT_log  D_Bil_log   ALP_log  AST_GOT_log  B_Bil_log  \\\n",
       "0      0.089946     0.237739   0.223862  0.085380     0.316059   0.426557   \n",
       "1      0.135594     0.128030   0.244362  0.114341     0.023488   0.456205   \n",
       "2      0.202945     0.113081   0.371615  0.058837     0.012537   0.472039   \n",
       "3      0.121448     0.140774   0.185741  0.118829     0.091682   0.456470   \n",
       "4      0.328243     0.216428   0.451248  0.132027     0.408537   0.548603   \n",
       "...         ...          ...        ...       ...          ...        ...   \n",
       "3495   0.150118     0.198362   0.250888  0.100623     0.120821   0.465609   \n",
       "3496   0.580763     0.247009   0.662246  0.522152     0.322759   0.677669   \n",
       "3497   0.073858     0.216871   0.229708  0.080811     0.185622   0.412876   \n",
       "3498   0.336230     0.163243   0.291304  0.302112     0.366636   0.591029   \n",
       "3499   0.308792     0.259199   0.393897  0.310660     0.290208   0.551986   \n",
       "\n",
       "           Alb    TP_log   ALT_GPT       Age  ...  AST_ALT_ratio_log  \\\n",
       "0     0.603580  0.679707  0.011540  0.541667  ...           0.565627   \n",
       "1     0.573186  0.717510  0.004393  0.861111  ...           0.465377   \n",
       "2     0.573095  0.804082  0.003709  0.361111  ...           0.471926   \n",
       "3     0.552490  0.732406  0.005021  0.861111  ...           0.500932   \n",
       "4     0.240197  0.695803  0.009799  0.777778  ...           0.650512   \n",
       "...        ...       ...       ...       ...  ...                ...   \n",
       "3495  0.815802  0.941273  0.008474  0.625000  ...           0.466581   \n",
       "3496  0.224272  0.721254  0.012362  0.583333  ...           0.561502   \n",
       "3497  0.099244  0.506385  0.009833  0.861111  ...           0.494287   \n",
       "3498  0.053591  0.194500  0.006241  0.777778  ...           0.671759   \n",
       "3499  0.853258  0.674075  0.013508  0.250000  ...           0.527168   \n",
       "\n",
       "       AST_GOT  AG_ratio  AST_ALT_ratio  Female        TP  AG_ratio_log  \\\n",
       "0     0.047381  0.187176       0.080215     0.0  0.578569      0.390896   \n",
       "1     0.001837  0.257358       0.043760     1.0  0.622254      0.483838   \n",
       "2     0.000959  0.305783       0.045557     0.0  0.728047      0.539257   \n",
       "3     0.008262  0.257125       0.054376     1.0  0.639878      0.483555   \n",
       "4     0.076940  0.259223       0.132447     0.0  0.596989      0.486088   \n",
       "...        ...       ...            ...     ...       ...           ...   \n",
       "3495  0.011589  0.286878       0.044085     1.0  0.913421      0.518339   \n",
       "3496  0.049175  0.048812       0.078267     0.0  0.626662      0.134320   \n",
       "3497  0.020537  0.017626       0.052225     0.0  0.396156      0.052715   \n",
       "3498  0.062188  0.011676       0.149998     0.0  0.131381      0.035529   \n",
       "3499  0.040894  0.268835       0.063718     0.0  0.572186      0.497532   \n",
       "\n",
       "           ALP     D_Bil     B_Bil  \n",
       "0     0.019046  0.007005  0.395024  \n",
       "1     0.026558  0.008247  0.398277  \n",
       "2     0.012654  0.020544  0.400286  \n",
       "3     0.027774  0.005066  0.398309  \n",
       "4     0.031439  0.034659  0.413492  \n",
       "...        ...       ...       ...  \n",
       "3495  0.022927  0.008675  0.399445  \n",
       "3496  0.225528  0.129379  0.457451  \n",
       "3497  0.017913  0.007344  0.393720  \n",
       "3498  0.092271  0.011737  0.424150  \n",
       "3499  0.096119  0.023840  0.414239  \n",
       "\n",
       "[3500 rows x 23 columns]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_imp, y, test_size=0.25, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "474 43\n",
      "165 193\n",
      "correct: 667 incorrect: 208\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "y_pred_lg_val = logReg_model.predict(X_val)\n",
    "a,b,c,d = confusion_matrix(y_val, y_pred_lg_val).ravel()\n",
    "print(a, b)\n",
    "print(c, d)\n",
    "print(\"correct:\",str(a+d), \"incorrect:\",str(b+c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "494 23\n",
      "209 149\n",
      "correct: 643 incorrect: 232\n"
     ]
    }
   ],
   "source": [
    "# Naives Bayes\n",
    "y_pred_naive_val = naiveBayes_model.predict(X_val)\n",
    "a,b,c,d = confusion_matrix(y_val, y_pred_naive_val).ravel()\n",
    "print(a, b)\n",
    "print(c, d)\n",
    "print(\"correct:\",str(a+d), \"incorrect:\",str(b+c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "452 65\n",
      "63 295\n",
      "correct: 747 incorrect: 128\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "y_pred_rf_val = rf_model.predict(X_val)\n",
    "a,b,c,d = confusion_matrix(y_val, y_pred_rf_val).ravel()\n",
    "print(a, b)\n",
    "print(c, d)\n",
    "print(\"correct:\",str(a+d), \"incorrect:\",str(b+c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "476 41\n",
      "160 198\n",
      "correct: 674 incorrect: 201\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Classification\n",
    "y_pred_svc_val = svc_clf.predict(X_val)\n",
    "a,b,c,d = confusion_matrix(y_val, y_pred_svc_val).ravel()\n",
    "print(a, b)\n",
    "print(c, d)\n",
    "print(\"correct:\",str(a+d), \"incorrect:\",str(b+c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "458 59\n",
      "69 289\n",
      "correct: 747 incorrect: 128\n"
     ]
    }
   ],
   "source": [
    "# Extra Trees Classifier\n",
    "y_pred_et_val = et_clf.predict(X_val)\n",
    "a,b,c,d = confusion_matrix(y_val, y_pred_et_val).ravel()\n",
    "print(a, b)\n",
    "print(c, d)\n",
    "print(\"correct:\",str(a+d), \"incorrect:\",str(b+c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X. Training the Model (Sequential Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=seed)\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Dropout\n",
    "\n",
    "input_dim = X_train.shape[1]  # Number of features\n",
    "\n",
    "seq_model = Sequential()\n",
    "seq_model.add(Dense(10, input_dim=input_dim, activation='relu'))\n",
    "seq_model.add(Dense(20, activation='relu'))\n",
    "seq_model.add(Dropout(0.1))\n",
    "seq_model.add(Dense(40, activation='relu'))\n",
    "seq_model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 10)                250       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 20)                220       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 40)                840       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 41        \n",
      "=================================================================\n",
      "Total params: 1,351\n",
      "Trainable params: 1,351\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "seq_model.compile(loss='binary_crossentropy', \n",
    "                  optimizer='adam', \n",
    "                  metrics=['accuracy'])\n",
    "seq_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "6/6 [==============================] - 1s 31ms/step - loss: 0.6852 - accuracy: 0.5844 - val_loss: 0.6746 - val_accuracy: 0.5909\n",
      "Epoch 2/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6757 - accuracy: 0.5844 - val_loss: 0.6641 - val_accuracy: 0.5909\n",
      "Epoch 3/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6666 - accuracy: 0.5844 - val_loss: 0.6536 - val_accuracy: 0.5909\n",
      "Epoch 4/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6587 - accuracy: 0.5878 - val_loss: 0.6441 - val_accuracy: 0.5909\n",
      "Epoch 5/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6502 - accuracy: 0.5954 - val_loss: 0.6346 - val_accuracy: 0.6263\n",
      "Epoch 6/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6395 - accuracy: 0.6286 - val_loss: 0.6230 - val_accuracy: 0.6537\n",
      "Epoch 7/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6313 - accuracy: 0.6491 - val_loss: 0.6112 - val_accuracy: 0.6926\n",
      "Epoch 8/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6199 - accuracy: 0.6861 - val_loss: 0.5986 - val_accuracy: 0.7051\n",
      "Epoch 9/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6122 - accuracy: 0.6823 - val_loss: 0.5872 - val_accuracy: 0.7086\n",
      "Epoch 10/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6015 - accuracy: 0.6998 - val_loss: 0.5747 - val_accuracy: 0.7200\n",
      "Epoch 11/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5879 - accuracy: 0.7166 - val_loss: 0.5641 - val_accuracy: 0.7440\n",
      "Epoch 12/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5815 - accuracy: 0.7162 - val_loss: 0.5520 - val_accuracy: 0.7497\n",
      "Epoch 13/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5753 - accuracy: 0.7154 - val_loss: 0.5417 - val_accuracy: 0.7280\n",
      "Epoch 14/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5636 - accuracy: 0.7170 - val_loss: 0.5322 - val_accuracy: 0.7520\n",
      "Epoch 15/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5587 - accuracy: 0.7173 - val_loss: 0.5256 - val_accuracy: 0.7509\n",
      "Epoch 16/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5516 - accuracy: 0.7250 - val_loss: 0.5173 - val_accuracy: 0.7474\n",
      "Epoch 17/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5432 - accuracy: 0.7269 - val_loss: 0.5110 - val_accuracy: 0.7566\n",
      "Epoch 18/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.5412 - accuracy: 0.7326 - val_loss: 0.5074 - val_accuracy: 0.7577\n",
      "Epoch 19/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5394 - accuracy: 0.7272 - val_loss: 0.5012 - val_accuracy: 0.7520\n",
      "Epoch 20/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.5398 - accuracy: 0.7288 - val_loss: 0.4971 - val_accuracy: 0.7531\n",
      "Epoch 21/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5293 - accuracy: 0.7364 - val_loss: 0.4946 - val_accuracy: 0.7600\n",
      "Epoch 22/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5287 - accuracy: 0.7387 - val_loss: 0.4911 - val_accuracy: 0.7589\n",
      "Epoch 23/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5174 - accuracy: 0.7432 - val_loss: 0.4882 - val_accuracy: 0.7589\n",
      "Epoch 24/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5223 - accuracy: 0.7390 - val_loss: 0.4853 - val_accuracy: 0.7657\n",
      "Epoch 25/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5180 - accuracy: 0.7406 - val_loss: 0.4836 - val_accuracy: 0.7623\n",
      "Epoch 26/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5191 - accuracy: 0.7360 - val_loss: 0.4804 - val_accuracy: 0.7669\n",
      "Epoch 27/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5138 - accuracy: 0.7451 - val_loss: 0.4789 - val_accuracy: 0.7680\n",
      "Epoch 28/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5151 - accuracy: 0.7474 - val_loss: 0.4782 - val_accuracy: 0.7680\n",
      "Epoch 29/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5191 - accuracy: 0.7455 - val_loss: 0.4771 - val_accuracy: 0.7714\n",
      "Epoch 30/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5073 - accuracy: 0.7463 - val_loss: 0.4756 - val_accuracy: 0.7680\n",
      "Epoch 31/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.5126 - accuracy: 0.7516 - val_loss: 0.4730 - val_accuracy: 0.7680\n",
      "Epoch 32/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5035 - accuracy: 0.7455 - val_loss: 0.4711 - val_accuracy: 0.7726\n",
      "Epoch 33/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.5058 - accuracy: 0.7520 - val_loss: 0.4693 - val_accuracy: 0.7714\n",
      "Epoch 34/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5018 - accuracy: 0.7554 - val_loss: 0.4674 - val_accuracy: 0.7714\n",
      "Epoch 35/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5009 - accuracy: 0.7562 - val_loss: 0.4663 - val_accuracy: 0.7726\n",
      "Epoch 36/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4963 - accuracy: 0.7539 - val_loss: 0.4653 - val_accuracy: 0.7737\n",
      "Epoch 37/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4955 - accuracy: 0.7509 - val_loss: 0.4643 - val_accuracy: 0.7749\n",
      "Epoch 38/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4968 - accuracy: 0.7550 - val_loss: 0.4626 - val_accuracy: 0.7737\n",
      "Epoch 39/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4964 - accuracy: 0.7562 - val_loss: 0.4615 - val_accuracy: 0.7726\n",
      "Epoch 40/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4971 - accuracy: 0.7509 - val_loss: 0.4604 - val_accuracy: 0.7749\n",
      "Epoch 41/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4920 - accuracy: 0.7562 - val_loss: 0.4606 - val_accuracy: 0.7760\n",
      "Epoch 42/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4924 - accuracy: 0.7596 - val_loss: 0.4596 - val_accuracy: 0.7794\n",
      "Epoch 43/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4942 - accuracy: 0.7604 - val_loss: 0.4585 - val_accuracy: 0.7806\n",
      "Epoch 44/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4873 - accuracy: 0.7630 - val_loss: 0.4577 - val_accuracy: 0.7783\n",
      "Epoch 45/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4858 - accuracy: 0.7562 - val_loss: 0.4569 - val_accuracy: 0.7840\n",
      "Epoch 46/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4881 - accuracy: 0.7642 - val_loss: 0.4555 - val_accuracy: 0.7886\n",
      "Epoch 47/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4886 - accuracy: 0.7627 - val_loss: 0.4538 - val_accuracy: 0.7840\n",
      "Epoch 48/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4894 - accuracy: 0.7592 - val_loss: 0.4525 - val_accuracy: 0.7851\n",
      "Epoch 49/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4877 - accuracy: 0.7608 - val_loss: 0.4520 - val_accuracy: 0.7863\n",
      "Epoch 50/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4843 - accuracy: 0.7691 - val_loss: 0.4521 - val_accuracy: 0.7897\n",
      "Epoch 51/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4850 - accuracy: 0.7676 - val_loss: 0.4521 - val_accuracy: 0.7874\n",
      "Epoch 52/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4866 - accuracy: 0.7596 - val_loss: 0.4511 - val_accuracy: 0.7863\n",
      "Epoch 53/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4804 - accuracy: 0.7634 - val_loss: 0.4499 - val_accuracy: 0.7874\n",
      "Epoch 54/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4798 - accuracy: 0.7627 - val_loss: 0.4485 - val_accuracy: 0.7897\n",
      "Epoch 55/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4786 - accuracy: 0.7642 - val_loss: 0.4490 - val_accuracy: 0.7977\n",
      "Epoch 56/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4800 - accuracy: 0.7703 - val_loss: 0.4469 - val_accuracy: 0.7909\n",
      "Epoch 57/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4754 - accuracy: 0.7669 - val_loss: 0.4460 - val_accuracy: 0.7920\n",
      "Epoch 58/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4805 - accuracy: 0.7661 - val_loss: 0.4459 - val_accuracy: 0.7920\n",
      "Epoch 59/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4731 - accuracy: 0.7676 - val_loss: 0.4461 - val_accuracy: 0.7840\n",
      "Epoch 60/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4782 - accuracy: 0.7600 - val_loss: 0.4444 - val_accuracy: 0.7886\n",
      "Epoch 61/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4742 - accuracy: 0.7756 - val_loss: 0.4485 - val_accuracy: 0.7966\n",
      "Epoch 62/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4718 - accuracy: 0.7703 - val_loss: 0.4452 - val_accuracy: 0.7829\n",
      "Epoch 63/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4736 - accuracy: 0.7650 - val_loss: 0.4429 - val_accuracy: 0.7920\n",
      "Epoch 64/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4700 - accuracy: 0.7779 - val_loss: 0.4423 - val_accuracy: 0.7897\n",
      "Epoch 65/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4653 - accuracy: 0.7741 - val_loss: 0.4428 - val_accuracy: 0.7829\n",
      "Epoch 66/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4698 - accuracy: 0.7756 - val_loss: 0.4424 - val_accuracy: 0.7920\n",
      "Epoch 67/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4671 - accuracy: 0.7787 - val_loss: 0.4410 - val_accuracy: 0.7909\n",
      "Epoch 68/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4688 - accuracy: 0.7707 - val_loss: 0.4423 - val_accuracy: 0.7783\n",
      "Epoch 69/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4673 - accuracy: 0.7749 - val_loss: 0.4383 - val_accuracy: 0.7989\n",
      "Epoch 70/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4623 - accuracy: 0.7794 - val_loss: 0.4375 - val_accuracy: 0.7931\n",
      "Epoch 71/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4628 - accuracy: 0.7730 - val_loss: 0.4370 - val_accuracy: 0.7943\n",
      "Epoch 72/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4649 - accuracy: 0.7749 - val_loss: 0.4370 - val_accuracy: 0.7943\n",
      "Epoch 73/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4636 - accuracy: 0.7787 - val_loss: 0.4377 - val_accuracy: 0.7954\n",
      "Epoch 74/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4636 - accuracy: 0.7733 - val_loss: 0.4372 - val_accuracy: 0.7829\n",
      "Epoch 75/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4600 - accuracy: 0.7764 - val_loss: 0.4356 - val_accuracy: 0.7931\n",
      "Epoch 76/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4624 - accuracy: 0.7726 - val_loss: 0.4369 - val_accuracy: 0.7840\n",
      "Epoch 77/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4613 - accuracy: 0.7798 - val_loss: 0.4345 - val_accuracy: 0.7989\n",
      "Epoch 78/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4608 - accuracy: 0.7806 - val_loss: 0.4334 - val_accuracy: 0.7966\n",
      "Epoch 79/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4547 - accuracy: 0.7798 - val_loss: 0.4330 - val_accuracy: 0.7943\n",
      "Epoch 80/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4581 - accuracy: 0.7844 - val_loss: 0.4342 - val_accuracy: 0.7977\n",
      "Epoch 81/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4569 - accuracy: 0.7794 - val_loss: 0.4354 - val_accuracy: 0.7886\n",
      "Epoch 82/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4573 - accuracy: 0.7726 - val_loss: 0.4321 - val_accuracy: 0.7966\n",
      "Epoch 83/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4557 - accuracy: 0.7771 - val_loss: 0.4315 - val_accuracy: 0.7977\n",
      "Epoch 84/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4568 - accuracy: 0.7787 - val_loss: 0.4304 - val_accuracy: 0.7966\n",
      "Epoch 85/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4575 - accuracy: 0.7775 - val_loss: 0.4313 - val_accuracy: 0.7909\n",
      "Epoch 86/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4548 - accuracy: 0.7897 - val_loss: 0.4321 - val_accuracy: 0.7966\n",
      "Epoch 87/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4612 - accuracy: 0.7787 - val_loss: 0.4352 - val_accuracy: 0.7794\n",
      "Epoch 88/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4540 - accuracy: 0.7745 - val_loss: 0.4340 - val_accuracy: 0.8023\n",
      "Epoch 89/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4575 - accuracy: 0.7863 - val_loss: 0.4288 - val_accuracy: 0.7954\n",
      "Epoch 90/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4518 - accuracy: 0.7825 - val_loss: 0.4296 - val_accuracy: 0.7977\n",
      "Epoch 91/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4503 - accuracy: 0.7802 - val_loss: 0.4293 - val_accuracy: 0.8034\n",
      "Epoch 92/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4459 - accuracy: 0.7882 - val_loss: 0.4299 - val_accuracy: 0.7874\n",
      "Epoch 93/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4522 - accuracy: 0.7840 - val_loss: 0.4276 - val_accuracy: 0.7954\n",
      "Epoch 94/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4484 - accuracy: 0.7825 - val_loss: 0.4277 - val_accuracy: 0.8000\n",
      "Epoch 95/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4500 - accuracy: 0.7878 - val_loss: 0.4265 - val_accuracy: 0.8011\n",
      "Epoch 96/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4475 - accuracy: 0.7779 - val_loss: 0.4266 - val_accuracy: 0.8000\n",
      "Epoch 97/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4477 - accuracy: 0.7844 - val_loss: 0.4285 - val_accuracy: 0.7989\n",
      "Epoch 98/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4496 - accuracy: 0.7893 - val_loss: 0.4267 - val_accuracy: 0.7977\n",
      "Epoch 99/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4472 - accuracy: 0.7832 - val_loss: 0.4253 - val_accuracy: 0.8046\n",
      "Epoch 100/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4474 - accuracy: 0.7901 - val_loss: 0.4250 - val_accuracy: 0.8011\n",
      "Epoch 101/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4462 - accuracy: 0.7817 - val_loss: 0.4302 - val_accuracy: 0.7874\n",
      "Epoch 102/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4457 - accuracy: 0.7844 - val_loss: 0.4274 - val_accuracy: 0.7966\n",
      "Epoch 103/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4453 - accuracy: 0.7867 - val_loss: 0.4269 - val_accuracy: 0.7954\n",
      "Epoch 104/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4420 - accuracy: 0.7874 - val_loss: 0.4250 - val_accuracy: 0.7989\n",
      "Epoch 105/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4451 - accuracy: 0.7870 - val_loss: 0.4244 - val_accuracy: 0.8000\n",
      "Epoch 106/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4442 - accuracy: 0.7928 - val_loss: 0.4242 - val_accuracy: 0.7977\n",
      "Epoch 107/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4441 - accuracy: 0.7943 - val_loss: 0.4239 - val_accuracy: 0.7943\n",
      "Epoch 108/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4446 - accuracy: 0.7909 - val_loss: 0.4292 - val_accuracy: 0.7909\n",
      "Epoch 109/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4461 - accuracy: 0.7855 - val_loss: 0.4245 - val_accuracy: 0.8011\n",
      "Epoch 110/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4414 - accuracy: 0.7939 - val_loss: 0.4228 - val_accuracy: 0.7920\n",
      "Epoch 111/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4395 - accuracy: 0.7893 - val_loss: 0.4267 - val_accuracy: 0.7931\n",
      "Epoch 112/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4435 - accuracy: 0.7794 - val_loss: 0.4245 - val_accuracy: 0.7954\n",
      "Epoch 113/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4379 - accuracy: 0.7924 - val_loss: 0.4230 - val_accuracy: 0.7943\n",
      "Epoch 114/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4376 - accuracy: 0.7897 - val_loss: 0.4221 - val_accuracy: 0.7966\n",
      "Epoch 115/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4405 - accuracy: 0.7870 - val_loss: 0.4217 - val_accuracy: 0.7977\n",
      "Epoch 116/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4379 - accuracy: 0.7916 - val_loss: 0.4220 - val_accuracy: 0.8034\n",
      "Epoch 117/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4388 - accuracy: 0.7920 - val_loss: 0.4219 - val_accuracy: 0.7931\n",
      "Epoch 118/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4378 - accuracy: 0.7874 - val_loss: 0.4217 - val_accuracy: 0.7943\n",
      "Epoch 119/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4371 - accuracy: 0.7973 - val_loss: 0.4207 - val_accuracy: 0.8000\n",
      "Epoch 120/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4385 - accuracy: 0.7909 - val_loss: 0.4225 - val_accuracy: 0.7954\n",
      "Epoch 121/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4366 - accuracy: 0.7878 - val_loss: 0.4210 - val_accuracy: 0.7989\n",
      "Epoch 122/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4364 - accuracy: 0.7981 - val_loss: 0.4208 - val_accuracy: 0.8034\n",
      "Epoch 123/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4356 - accuracy: 0.7912 - val_loss: 0.4221 - val_accuracy: 0.7909\n",
      "Epoch 124/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4364 - accuracy: 0.7939 - val_loss: 0.4190 - val_accuracy: 0.8011\n",
      "Epoch 125/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4389 - accuracy: 0.7992 - val_loss: 0.4189 - val_accuracy: 0.8000\n",
      "Epoch 126/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4379 - accuracy: 0.7867 - val_loss: 0.4209 - val_accuracy: 0.7920\n",
      "Epoch 127/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4326 - accuracy: 0.7954 - val_loss: 0.4231 - val_accuracy: 0.8114\n",
      "Epoch 128/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4391 - accuracy: 0.7950 - val_loss: 0.4237 - val_accuracy: 0.7909\n",
      "Epoch 129/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4352 - accuracy: 0.7954 - val_loss: 0.4190 - val_accuracy: 0.7989\n",
      "Epoch 130/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4335 - accuracy: 0.7973 - val_loss: 0.4189 - val_accuracy: 0.7989\n",
      "Epoch 131/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4343 - accuracy: 0.7878 - val_loss: 0.4218 - val_accuracy: 0.7920\n",
      "Epoch 132/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4334 - accuracy: 0.7859 - val_loss: 0.4186 - val_accuracy: 0.7977\n",
      "Epoch 133/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4317 - accuracy: 0.7935 - val_loss: 0.4203 - val_accuracy: 0.7954\n",
      "Epoch 134/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4352 - accuracy: 0.7924 - val_loss: 0.4176 - val_accuracy: 0.8069\n",
      "Epoch 135/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4317 - accuracy: 0.7924 - val_loss: 0.4175 - val_accuracy: 0.8034\n",
      "Epoch 136/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4344 - accuracy: 0.7943 - val_loss: 0.4180 - val_accuracy: 0.7989\n",
      "Epoch 137/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4351 - accuracy: 0.7950 - val_loss: 0.4167 - val_accuracy: 0.8057\n",
      "Epoch 138/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4334 - accuracy: 0.7958 - val_loss: 0.4162 - val_accuracy: 0.8057\n",
      "Epoch 139/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4318 - accuracy: 0.7985 - val_loss: 0.4178 - val_accuracy: 0.8046\n",
      "Epoch 140/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4369 - accuracy: 0.7901 - val_loss: 0.4182 - val_accuracy: 0.8023\n",
      "Epoch 141/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4304 - accuracy: 0.7985 - val_loss: 0.4180 - val_accuracy: 0.8046\n",
      "Epoch 142/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4318 - accuracy: 0.7924 - val_loss: 0.4196 - val_accuracy: 0.8023\n",
      "Epoch 143/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4291 - accuracy: 0.7977 - val_loss: 0.4162 - val_accuracy: 0.8080\n",
      "Epoch 144/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4282 - accuracy: 0.7943 - val_loss: 0.4176 - val_accuracy: 0.8091\n",
      "Epoch 145/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4280 - accuracy: 0.7924 - val_loss: 0.4158 - val_accuracy: 0.8080\n",
      "Epoch 146/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4257 - accuracy: 0.7989 - val_loss: 0.4159 - val_accuracy: 0.8091\n",
      "Epoch 147/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4283 - accuracy: 0.7966 - val_loss: 0.4169 - val_accuracy: 0.8034\n",
      "Epoch 148/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4292 - accuracy: 0.7901 - val_loss: 0.4164 - val_accuracy: 0.8034\n",
      "Epoch 149/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4313 - accuracy: 0.7950 - val_loss: 0.4166 - val_accuracy: 0.8057\n",
      "Epoch 150/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4272 - accuracy: 0.7950 - val_loss: 0.4195 - val_accuracy: 0.8046\n",
      "Epoch 151/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4286 - accuracy: 0.7939 - val_loss: 0.4167 - val_accuracy: 0.8080\n",
      "Epoch 152/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4279 - accuracy: 0.7966 - val_loss: 0.4171 - val_accuracy: 0.8023\n",
      "Epoch 153/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4260 - accuracy: 0.7970 - val_loss: 0.4147 - val_accuracy: 0.8091\n",
      "Epoch 154/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4239 - accuracy: 0.8000 - val_loss: 0.4147 - val_accuracy: 0.8080\n",
      "Epoch 155/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4261 - accuracy: 0.7996 - val_loss: 0.4159 - val_accuracy: 0.8046\n",
      "Epoch 156/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4270 - accuracy: 0.7950 - val_loss: 0.4159 - val_accuracy: 0.8034\n",
      "Epoch 157/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4255 - accuracy: 0.7981 - val_loss: 0.4157 - val_accuracy: 0.8069\n",
      "Epoch 158/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4270 - accuracy: 0.7943 - val_loss: 0.4146 - val_accuracy: 0.8057\n",
      "Epoch 159/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4213 - accuracy: 0.8015 - val_loss: 0.4150 - val_accuracy: 0.8091\n",
      "Epoch 160/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4290 - accuracy: 0.7947 - val_loss: 0.4165 - val_accuracy: 0.8023\n",
      "Epoch 161/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4287 - accuracy: 0.7973 - val_loss: 0.4159 - val_accuracy: 0.8057\n",
      "Epoch 162/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4244 - accuracy: 0.7992 - val_loss: 0.4140 - val_accuracy: 0.8126\n",
      "Epoch 163/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4324 - accuracy: 0.8008 - val_loss: 0.4134 - val_accuracy: 0.8126\n",
      "Epoch 164/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4270 - accuracy: 0.7970 - val_loss: 0.4158 - val_accuracy: 0.8091\n",
      "Epoch 165/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4248 - accuracy: 0.7989 - val_loss: 0.4163 - val_accuracy: 0.8160\n",
      "Epoch 166/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4262 - accuracy: 0.7943 - val_loss: 0.4234 - val_accuracy: 0.7989\n",
      "Epoch 167/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4272 - accuracy: 0.7890 - val_loss: 0.4174 - val_accuracy: 0.8103\n",
      "Epoch 168/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4256 - accuracy: 0.8023 - val_loss: 0.4157 - val_accuracy: 0.8080\n",
      "Epoch 169/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4234 - accuracy: 0.7935 - val_loss: 0.4144 - val_accuracy: 0.8171\n",
      "Epoch 170/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4334 - accuracy: 0.7970 - val_loss: 0.4164 - val_accuracy: 0.8137\n",
      "Epoch 171/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4272 - accuracy: 0.7947 - val_loss: 0.4215 - val_accuracy: 0.7989\n",
      "Epoch 172/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4241 - accuracy: 0.7989 - val_loss: 0.4173 - val_accuracy: 0.8114\n",
      "Epoch 173/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4265 - accuracy: 0.7985 - val_loss: 0.4171 - val_accuracy: 0.8091\n",
      "Epoch 174/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4229 - accuracy: 0.7928 - val_loss: 0.4142 - val_accuracy: 0.8046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 175/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4183 - accuracy: 0.8030 - val_loss: 0.4131 - val_accuracy: 0.8057\n",
      "Epoch 176/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4253 - accuracy: 0.7985 - val_loss: 0.4128 - val_accuracy: 0.8091\n",
      "Epoch 177/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4200 - accuracy: 0.7985 - val_loss: 0.4127 - val_accuracy: 0.8080\n",
      "Epoch 178/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4246 - accuracy: 0.8011 - val_loss: 0.4143 - val_accuracy: 0.8080\n",
      "Epoch 179/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4197 - accuracy: 0.8008 - val_loss: 0.4147 - val_accuracy: 0.8034\n",
      "Epoch 180/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4206 - accuracy: 0.8008 - val_loss: 0.4123 - val_accuracy: 0.8160\n",
      "Epoch 181/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4215 - accuracy: 0.8019 - val_loss: 0.4141 - val_accuracy: 0.8057\n",
      "Epoch 182/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4236 - accuracy: 0.7950 - val_loss: 0.4139 - val_accuracy: 0.8023\n",
      "Epoch 183/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4218 - accuracy: 0.8050 - val_loss: 0.4124 - val_accuracy: 0.8057\n",
      "Epoch 184/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4224 - accuracy: 0.8011 - val_loss: 0.4130 - val_accuracy: 0.8069\n",
      "Epoch 185/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4207 - accuracy: 0.8042 - val_loss: 0.4154 - val_accuracy: 0.8091\n",
      "Epoch 186/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4240 - accuracy: 0.7985 - val_loss: 0.4142 - val_accuracy: 0.8103\n",
      "Epoch 187/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4209 - accuracy: 0.7973 - val_loss: 0.4146 - val_accuracy: 0.8114\n",
      "Epoch 188/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4231 - accuracy: 0.8015 - val_loss: 0.4178 - val_accuracy: 0.8034\n",
      "Epoch 189/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4229 - accuracy: 0.7966 - val_loss: 0.4143 - val_accuracy: 0.8091\n",
      "Epoch 190/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4205 - accuracy: 0.8019 - val_loss: 0.4128 - val_accuracy: 0.8137\n",
      "Epoch 191/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4182 - accuracy: 0.8004 - val_loss: 0.4145 - val_accuracy: 0.8126\n",
      "Epoch 192/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4200 - accuracy: 0.8023 - val_loss: 0.4124 - val_accuracy: 0.8137\n",
      "Epoch 193/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4180 - accuracy: 0.8061 - val_loss: 0.4133 - val_accuracy: 0.8114\n",
      "Epoch 194/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4203 - accuracy: 0.7981 - val_loss: 0.4117 - val_accuracy: 0.8149\n",
      "Epoch 195/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4184 - accuracy: 0.8034 - val_loss: 0.4130 - val_accuracy: 0.8103\n",
      "Epoch 196/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4212 - accuracy: 0.8030 - val_loss: 0.4138 - val_accuracy: 0.8103\n",
      "Epoch 197/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4175 - accuracy: 0.8034 - val_loss: 0.4124 - val_accuracy: 0.8160\n",
      "Epoch 198/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4184 - accuracy: 0.7977 - val_loss: 0.4131 - val_accuracy: 0.8160\n",
      "Epoch 199/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4163 - accuracy: 0.8065 - val_loss: 0.4118 - val_accuracy: 0.8183\n",
      "Epoch 200/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4151 - accuracy: 0.8069 - val_loss: 0.4122 - val_accuracy: 0.8160\n",
      "Epoch 201/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4156 - accuracy: 0.7989 - val_loss: 0.4119 - val_accuracy: 0.8149\n",
      "Epoch 202/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4173 - accuracy: 0.8019 - val_loss: 0.4125 - val_accuracy: 0.8137\n",
      "Epoch 203/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4159 - accuracy: 0.8030 - val_loss: 0.4113 - val_accuracy: 0.8126\n",
      "Epoch 204/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4178 - accuracy: 0.8050 - val_loss: 0.4120 - val_accuracy: 0.8171\n",
      "Epoch 205/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4137 - accuracy: 0.8034 - val_loss: 0.4119 - val_accuracy: 0.8149\n",
      "Epoch 206/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4167 - accuracy: 0.8061 - val_loss: 0.4116 - val_accuracy: 0.8149\n",
      "Epoch 207/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4164 - accuracy: 0.8038 - val_loss: 0.4114 - val_accuracy: 0.8114\n",
      "Epoch 208/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4169 - accuracy: 0.8030 - val_loss: 0.4129 - val_accuracy: 0.8160\n",
      "Epoch 209/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4234 - accuracy: 0.8023 - val_loss: 0.4117 - val_accuracy: 0.8171\n",
      "Epoch 210/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4202 - accuracy: 0.7977 - val_loss: 0.4108 - val_accuracy: 0.8160\n",
      "Epoch 211/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4180 - accuracy: 0.8000 - val_loss: 0.4116 - val_accuracy: 0.8137\n",
      "Epoch 212/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4179 - accuracy: 0.8042 - val_loss: 0.4099 - val_accuracy: 0.8217\n",
      "Epoch 213/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4157 - accuracy: 0.7996 - val_loss: 0.4156 - val_accuracy: 0.8091\n",
      "Epoch 214/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4158 - accuracy: 0.7989 - val_loss: 0.4115 - val_accuracy: 0.8149\n",
      "Epoch 215/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4175 - accuracy: 0.8030 - val_loss: 0.4102 - val_accuracy: 0.8194\n",
      "Epoch 216/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4132 - accuracy: 0.8107 - val_loss: 0.4090 - val_accuracy: 0.8240\n",
      "Epoch 217/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4122 - accuracy: 0.8072 - val_loss: 0.4113 - val_accuracy: 0.8126\n",
      "Epoch 218/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4133 - accuracy: 0.7989 - val_loss: 0.4097 - val_accuracy: 0.8137\n",
      "Epoch 219/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4196 - accuracy: 0.7989 - val_loss: 0.4096 - val_accuracy: 0.8206\n",
      "Epoch 220/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4165 - accuracy: 0.8091 - val_loss: 0.4100 - val_accuracy: 0.8183\n",
      "Epoch 221/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4162 - accuracy: 0.8027 - val_loss: 0.4146 - val_accuracy: 0.8137\n",
      "Epoch 222/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4179 - accuracy: 0.8042 - val_loss: 0.4115 - val_accuracy: 0.8206\n",
      "Epoch 223/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4133 - accuracy: 0.8114 - val_loss: 0.4200 - val_accuracy: 0.8069\n",
      "Epoch 224/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4159 - accuracy: 0.8080 - val_loss: 0.4116 - val_accuracy: 0.8160\n",
      "Epoch 225/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4139 - accuracy: 0.8095 - val_loss: 0.4141 - val_accuracy: 0.8137\n",
      "Epoch 226/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4162 - accuracy: 0.8076 - val_loss: 0.4087 - val_accuracy: 0.8206\n",
      "Epoch 227/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4155 - accuracy: 0.8023 - val_loss: 0.4099 - val_accuracy: 0.8206\n",
      "Epoch 228/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4211 - accuracy: 0.8023 - val_loss: 0.4105 - val_accuracy: 0.8183\n",
      "Epoch 229/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4189 - accuracy: 0.8069 - val_loss: 0.4093 - val_accuracy: 0.8194\n",
      "Epoch 230/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4169 - accuracy: 0.8050 - val_loss: 0.4225 - val_accuracy: 0.8069\n",
      "Epoch 231/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4172 - accuracy: 0.8076 - val_loss: 0.4136 - val_accuracy: 0.8126\n",
      "Epoch 232/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4154 - accuracy: 0.8034 - val_loss: 0.4153 - val_accuracy: 0.8137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 233/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4134 - accuracy: 0.8046 - val_loss: 0.4129 - val_accuracy: 0.8114\n",
      "Epoch 234/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4136 - accuracy: 0.8069 - val_loss: 0.4121 - val_accuracy: 0.8149\n",
      "Epoch 235/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4109 - accuracy: 0.8084 - val_loss: 0.4119 - val_accuracy: 0.8171\n",
      "Epoch 236/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4140 - accuracy: 0.8080 - val_loss: 0.4092 - val_accuracy: 0.8183\n",
      "Epoch 237/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4115 - accuracy: 0.8053 - val_loss: 0.4096 - val_accuracy: 0.8194\n",
      "Epoch 238/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4158 - accuracy: 0.8061 - val_loss: 0.4085 - val_accuracy: 0.8183\n",
      "Epoch 239/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4145 - accuracy: 0.8042 - val_loss: 0.4125 - val_accuracy: 0.8149\n",
      "Epoch 240/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4137 - accuracy: 0.8133 - val_loss: 0.4086 - val_accuracy: 0.8171\n",
      "Epoch 241/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4115 - accuracy: 0.8076 - val_loss: 0.4088 - val_accuracy: 0.8149\n",
      "Epoch 242/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4108 - accuracy: 0.8110 - val_loss: 0.4084 - val_accuracy: 0.8206\n",
      "Epoch 243/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4132 - accuracy: 0.8084 - val_loss: 0.4096 - val_accuracy: 0.8126\n",
      "Epoch 244/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4111 - accuracy: 0.8065 - val_loss: 0.4102 - val_accuracy: 0.8160\n",
      "Epoch 245/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4156 - accuracy: 0.8057 - val_loss: 0.4093 - val_accuracy: 0.8126\n",
      "Epoch 246/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4099 - accuracy: 0.8076 - val_loss: 0.4168 - val_accuracy: 0.8114\n",
      "Epoch 247/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4118 - accuracy: 0.8061 - val_loss: 0.4099 - val_accuracy: 0.8183\n",
      "Epoch 248/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4189 - accuracy: 0.8095 - val_loss: 0.4106 - val_accuracy: 0.8171\n",
      "Epoch 249/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4095 - accuracy: 0.8038 - val_loss: 0.4116 - val_accuracy: 0.8149\n",
      "Epoch 250/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4098 - accuracy: 0.8076 - val_loss: 0.4087 - val_accuracy: 0.8229\n",
      "Epoch 251/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4066 - accuracy: 0.8126 - val_loss: 0.4164 - val_accuracy: 0.8137\n",
      "Epoch 252/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4086 - accuracy: 0.8042 - val_loss: 0.4093 - val_accuracy: 0.8137\n",
      "Epoch 253/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4089 - accuracy: 0.8057 - val_loss: 0.4110 - val_accuracy: 0.8160\n",
      "Epoch 254/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4123 - accuracy: 0.8065 - val_loss: 0.4086 - val_accuracy: 0.8160\n",
      "Epoch 255/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4111 - accuracy: 0.8038 - val_loss: 0.4085 - val_accuracy: 0.8160\n",
      "Epoch 256/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4102 - accuracy: 0.8061 - val_loss: 0.4085 - val_accuracy: 0.8171\n",
      "Epoch 257/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4280 - accuracy: 0.7981 - val_loss: 0.4086 - val_accuracy: 0.8171\n",
      "Epoch 258/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4137 - accuracy: 0.8065 - val_loss: 0.4158 - val_accuracy: 0.8149\n",
      "Epoch 259/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4114 - accuracy: 0.8088 - val_loss: 0.4097 - val_accuracy: 0.8171\n",
      "Epoch 260/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4137 - accuracy: 0.8011 - val_loss: 0.4087 - val_accuracy: 0.8160\n",
      "Epoch 261/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4093 - accuracy: 0.8084 - val_loss: 0.4096 - val_accuracy: 0.8103\n",
      "Epoch 262/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4108 - accuracy: 0.8122 - val_loss: 0.4126 - val_accuracy: 0.8114\n",
      "Epoch 263/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4109 - accuracy: 0.8050 - val_loss: 0.4072 - val_accuracy: 0.8194\n",
      "Epoch 264/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4135 - accuracy: 0.8080 - val_loss: 0.4073 - val_accuracy: 0.8206\n",
      "Epoch 265/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4128 - accuracy: 0.8038 - val_loss: 0.4129 - val_accuracy: 0.8137\n",
      "Epoch 266/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4050 - accuracy: 0.8095 - val_loss: 0.4085 - val_accuracy: 0.8149\n",
      "Epoch 267/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4100 - accuracy: 0.8084 - val_loss: 0.4102 - val_accuracy: 0.8160\n",
      "Epoch 268/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4077 - accuracy: 0.8072 - val_loss: 0.4118 - val_accuracy: 0.8069\n",
      "Epoch 269/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4088 - accuracy: 0.8095 - val_loss: 0.4138 - val_accuracy: 0.8114\n",
      "Epoch 270/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4108 - accuracy: 0.8099 - val_loss: 0.4098 - val_accuracy: 0.8160\n",
      "Epoch 271/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4076 - accuracy: 0.8088 - val_loss: 0.4089 - val_accuracy: 0.8160\n",
      "Epoch 272/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4052 - accuracy: 0.8122 - val_loss: 0.4098 - val_accuracy: 0.8149\n",
      "Epoch 273/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4075 - accuracy: 0.8095 - val_loss: 0.4081 - val_accuracy: 0.8126\n",
      "Epoch 274/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4081 - accuracy: 0.8088 - val_loss: 0.4075 - val_accuracy: 0.8194\n",
      "Epoch 275/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4050 - accuracy: 0.8061 - val_loss: 0.4082 - val_accuracy: 0.8183\n",
      "Epoch 276/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4070 - accuracy: 0.8110 - val_loss: 0.4077 - val_accuracy: 0.8206\n",
      "Epoch 277/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4084 - accuracy: 0.8061 - val_loss: 0.4086 - val_accuracy: 0.8183\n",
      "Epoch 278/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4060 - accuracy: 0.8141 - val_loss: 0.4099 - val_accuracy: 0.8137\n",
      "Epoch 279/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4069 - accuracy: 0.8069 - val_loss: 0.4088 - val_accuracy: 0.8160\n",
      "Epoch 280/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4023 - accuracy: 0.8110 - val_loss: 0.4077 - val_accuracy: 0.8206\n",
      "Epoch 281/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4043 - accuracy: 0.8050 - val_loss: 0.4110 - val_accuracy: 0.8183\n",
      "Epoch 282/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4042 - accuracy: 0.8084 - val_loss: 0.4088 - val_accuracy: 0.8149\n",
      "Epoch 283/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4037 - accuracy: 0.8133 - val_loss: 0.4082 - val_accuracy: 0.8183\n",
      "Epoch 284/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4076 - accuracy: 0.8091 - val_loss: 0.4079 - val_accuracy: 0.8206\n",
      "Epoch 285/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4031 - accuracy: 0.8061 - val_loss: 0.4094 - val_accuracy: 0.8137\n",
      "Epoch 286/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4051 - accuracy: 0.8103 - val_loss: 0.4083 - val_accuracy: 0.8114\n",
      "Epoch 287/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4077 - accuracy: 0.8103 - val_loss: 0.4112 - val_accuracy: 0.8126\n",
      "Epoch 288/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4074 - accuracy: 0.8099 - val_loss: 0.4089 - val_accuracy: 0.8126\n",
      "Epoch 289/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4031 - accuracy: 0.8103 - val_loss: 0.4093 - val_accuracy: 0.8137\n",
      "Epoch 290/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4024 - accuracy: 0.8072 - val_loss: 0.4087 - val_accuracy: 0.8240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 291/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4037 - accuracy: 0.8099 - val_loss: 0.4093 - val_accuracy: 0.8183\n",
      "Epoch 292/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4061 - accuracy: 0.8088 - val_loss: 0.4115 - val_accuracy: 0.8137\n",
      "Epoch 293/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4028 - accuracy: 0.8133 - val_loss: 0.4086 - val_accuracy: 0.8171\n",
      "Epoch 294/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4025 - accuracy: 0.8061 - val_loss: 0.4096 - val_accuracy: 0.8251\n",
      "Epoch 295/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4002 - accuracy: 0.8110 - val_loss: 0.4097 - val_accuracy: 0.8171\n",
      "Epoch 296/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4015 - accuracy: 0.8088 - val_loss: 0.4105 - val_accuracy: 0.8091\n",
      "Epoch 297/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4022 - accuracy: 0.8076 - val_loss: 0.4096 - val_accuracy: 0.8206\n",
      "Epoch 298/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4051 - accuracy: 0.8046 - val_loss: 0.4090 - val_accuracy: 0.8103\n",
      "Epoch 299/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4062 - accuracy: 0.8069 - val_loss: 0.4099 - val_accuracy: 0.8171\n",
      "Epoch 300/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4054 - accuracy: 0.8095 - val_loss: 0.4111 - val_accuracy: 0.8149\n",
      "Epoch 301/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4074 - accuracy: 0.8065 - val_loss: 0.4112 - val_accuracy: 0.8057\n",
      "Epoch 302/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4056 - accuracy: 0.8084 - val_loss: 0.4085 - val_accuracy: 0.8080\n",
      "Epoch 303/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4006 - accuracy: 0.8149 - val_loss: 0.4063 - val_accuracy: 0.8183\n",
      "Epoch 304/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4019 - accuracy: 0.8103 - val_loss: 0.4062 - val_accuracy: 0.8160\n",
      "Epoch 305/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4050 - accuracy: 0.8107 - val_loss: 0.4099 - val_accuracy: 0.8171\n",
      "Epoch 306/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4023 - accuracy: 0.8088 - val_loss: 0.4068 - val_accuracy: 0.8114\n",
      "Epoch 307/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4039 - accuracy: 0.8084 - val_loss: 0.4062 - val_accuracy: 0.8206\n",
      "Epoch 308/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4013 - accuracy: 0.8099 - val_loss: 0.4092 - val_accuracy: 0.8194\n",
      "Epoch 309/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3990 - accuracy: 0.8107 - val_loss: 0.4065 - val_accuracy: 0.8114\n",
      "Epoch 310/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4037 - accuracy: 0.8095 - val_loss: 0.4073 - val_accuracy: 0.8103\n",
      "Epoch 311/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4038 - accuracy: 0.8126 - val_loss: 0.4068 - val_accuracy: 0.8183\n",
      "Epoch 312/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4010 - accuracy: 0.8088 - val_loss: 0.4057 - val_accuracy: 0.8160\n",
      "Epoch 313/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4037 - accuracy: 0.8057 - val_loss: 0.4060 - val_accuracy: 0.8149\n",
      "Epoch 314/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4042 - accuracy: 0.8069 - val_loss: 0.4053 - val_accuracy: 0.8206\n",
      "Epoch 315/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4026 - accuracy: 0.8152 - val_loss: 0.4066 - val_accuracy: 0.8080\n",
      "Epoch 316/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4039 - accuracy: 0.8126 - val_loss: 0.4080 - val_accuracy: 0.8103\n",
      "Epoch 317/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3995 - accuracy: 0.8122 - val_loss: 0.4065 - val_accuracy: 0.8171\n",
      "Epoch 318/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4029 - accuracy: 0.8076 - val_loss: 0.4131 - val_accuracy: 0.8171\n",
      "Epoch 319/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4020 - accuracy: 0.8130 - val_loss: 0.4064 - val_accuracy: 0.8194\n",
      "Epoch 320/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3999 - accuracy: 0.8118 - val_loss: 0.4086 - val_accuracy: 0.8149\n",
      "Epoch 321/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3974 - accuracy: 0.8084 - val_loss: 0.4061 - val_accuracy: 0.8171\n",
      "Epoch 322/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4002 - accuracy: 0.8084 - val_loss: 0.4076 - val_accuracy: 0.8160\n",
      "Epoch 323/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4039 - accuracy: 0.8099 - val_loss: 0.4067 - val_accuracy: 0.8137\n",
      "Epoch 324/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4007 - accuracy: 0.8141 - val_loss: 0.4060 - val_accuracy: 0.8137\n",
      "Epoch 325/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4008 - accuracy: 0.8145 - val_loss: 0.4121 - val_accuracy: 0.8126\n",
      "Epoch 326/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4049 - accuracy: 0.8061 - val_loss: 0.4079 - val_accuracy: 0.8126\n",
      "Epoch 327/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4042 - accuracy: 0.8099 - val_loss: 0.4072 - val_accuracy: 0.8206\n",
      "Epoch 328/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3996 - accuracy: 0.8114 - val_loss: 0.4053 - val_accuracy: 0.8240\n",
      "Epoch 329/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3974 - accuracy: 0.8103 - val_loss: 0.4051 - val_accuracy: 0.8194\n",
      "Epoch 330/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4010 - accuracy: 0.8076 - val_loss: 0.4054 - val_accuracy: 0.8251\n",
      "Epoch 331/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3981 - accuracy: 0.8114 - val_loss: 0.4078 - val_accuracy: 0.8194\n",
      "Epoch 332/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4003 - accuracy: 0.8118 - val_loss: 0.4086 - val_accuracy: 0.8126\n",
      "Epoch 333/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4009 - accuracy: 0.8080 - val_loss: 0.4100 - val_accuracy: 0.8149\n",
      "Epoch 334/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4008 - accuracy: 0.8103 - val_loss: 0.4056 - val_accuracy: 0.8160\n",
      "Epoch 335/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4008 - accuracy: 0.8149 - val_loss: 0.4077 - val_accuracy: 0.8126\n",
      "Epoch 336/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4014 - accuracy: 0.8076 - val_loss: 0.4062 - val_accuracy: 0.8194\n",
      "Epoch 337/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3968 - accuracy: 0.8099 - val_loss: 0.4039 - val_accuracy: 0.8229\n",
      "Epoch 338/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4003 - accuracy: 0.8103 - val_loss: 0.4059 - val_accuracy: 0.8217\n",
      "Epoch 339/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3966 - accuracy: 0.8152 - val_loss: 0.4067 - val_accuracy: 0.8149\n",
      "Epoch 340/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3972 - accuracy: 0.8152 - val_loss: 0.4075 - val_accuracy: 0.8183\n",
      "Epoch 341/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3971 - accuracy: 0.8130 - val_loss: 0.4050 - val_accuracy: 0.8171\n",
      "Epoch 342/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3950 - accuracy: 0.8130 - val_loss: 0.4088 - val_accuracy: 0.8194\n",
      "Epoch 343/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4000 - accuracy: 0.8149 - val_loss: 0.4054 - val_accuracy: 0.8149\n",
      "Epoch 344/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3968 - accuracy: 0.8175 - val_loss: 0.4063 - val_accuracy: 0.8217\n",
      "Epoch 345/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3970 - accuracy: 0.8088 - val_loss: 0.4069 - val_accuracy: 0.8194\n",
      "Epoch 346/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3965 - accuracy: 0.8137 - val_loss: 0.4076 - val_accuracy: 0.8126\n",
      "Epoch 347/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4039 - accuracy: 0.8107 - val_loss: 0.4067 - val_accuracy: 0.8171\n",
      "Epoch 348/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3972 - accuracy: 0.8156 - val_loss: 0.4053 - val_accuracy: 0.8160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 349/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4005 - accuracy: 0.8080 - val_loss: 0.4071 - val_accuracy: 0.8206\n",
      "Epoch 350/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3968 - accuracy: 0.8088 - val_loss: 0.4049 - val_accuracy: 0.8206\n",
      "Epoch 351/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3989 - accuracy: 0.8110 - val_loss: 0.4049 - val_accuracy: 0.8251\n",
      "Epoch 352/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3952 - accuracy: 0.8095 - val_loss: 0.4064 - val_accuracy: 0.8251\n",
      "Epoch 353/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3957 - accuracy: 0.8072 - val_loss: 0.4057 - val_accuracy: 0.8217\n",
      "Epoch 354/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3937 - accuracy: 0.8141 - val_loss: 0.4051 - val_accuracy: 0.8194\n",
      "Epoch 355/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3968 - accuracy: 0.8141 - val_loss: 0.4042 - val_accuracy: 0.8183\n",
      "Epoch 356/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3980 - accuracy: 0.8122 - val_loss: 0.4038 - val_accuracy: 0.8160\n",
      "Epoch 357/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3968 - accuracy: 0.8160 - val_loss: 0.4056 - val_accuracy: 0.8217\n",
      "Epoch 358/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3979 - accuracy: 0.8130 - val_loss: 0.4043 - val_accuracy: 0.8183\n",
      "Epoch 359/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3955 - accuracy: 0.8160 - val_loss: 0.4033 - val_accuracy: 0.8206\n",
      "Epoch 360/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3938 - accuracy: 0.8141 - val_loss: 0.4022 - val_accuracy: 0.8229\n",
      "Epoch 361/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3942 - accuracy: 0.8133 - val_loss: 0.4027 - val_accuracy: 0.8229\n",
      "Epoch 362/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3926 - accuracy: 0.8130 - val_loss: 0.4054 - val_accuracy: 0.8240\n",
      "Epoch 363/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3980 - accuracy: 0.8084 - val_loss: 0.4067 - val_accuracy: 0.8217\n",
      "Epoch 364/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3974 - accuracy: 0.8110 - val_loss: 0.4056 - val_accuracy: 0.8194\n",
      "Epoch 365/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3965 - accuracy: 0.8038 - val_loss: 0.4050 - val_accuracy: 0.8229\n",
      "Epoch 366/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3921 - accuracy: 0.8122 - val_loss: 0.4039 - val_accuracy: 0.8217\n",
      "Epoch 367/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3950 - accuracy: 0.8053 - val_loss: 0.4031 - val_accuracy: 0.8137\n",
      "Epoch 368/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3985 - accuracy: 0.8130 - val_loss: 0.4033 - val_accuracy: 0.8217\n",
      "Epoch 369/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3948 - accuracy: 0.8156 - val_loss: 0.4041 - val_accuracy: 0.8229\n",
      "Epoch 370/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3940 - accuracy: 0.8133 - val_loss: 0.4038 - val_accuracy: 0.8171\n",
      "Epoch 371/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3937 - accuracy: 0.8137 - val_loss: 0.4058 - val_accuracy: 0.8229\n",
      "Epoch 372/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3922 - accuracy: 0.8137 - val_loss: 0.4025 - val_accuracy: 0.8251\n",
      "Epoch 373/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3968 - accuracy: 0.8126 - val_loss: 0.4067 - val_accuracy: 0.8171\n",
      "Epoch 374/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3950 - accuracy: 0.8149 - val_loss: 0.4032 - val_accuracy: 0.8229\n",
      "Epoch 375/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3919 - accuracy: 0.8145 - val_loss: 0.4022 - val_accuracy: 0.8263\n",
      "Epoch 376/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3960 - accuracy: 0.8141 - val_loss: 0.4032 - val_accuracy: 0.8240\n",
      "Epoch 377/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4013 - accuracy: 0.8091 - val_loss: 0.4021 - val_accuracy: 0.8274\n",
      "Epoch 378/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3943 - accuracy: 0.8133 - val_loss: 0.4014 - val_accuracy: 0.8229\n",
      "Epoch 379/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3931 - accuracy: 0.8095 - val_loss: 0.4047 - val_accuracy: 0.8206\n",
      "Epoch 380/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3941 - accuracy: 0.8137 - val_loss: 0.4026 - val_accuracy: 0.8160\n",
      "Epoch 381/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3968 - accuracy: 0.8145 - val_loss: 0.4044 - val_accuracy: 0.8229\n",
      "Epoch 382/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3912 - accuracy: 0.8194 - val_loss: 0.4059 - val_accuracy: 0.8183\n",
      "Epoch 383/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3917 - accuracy: 0.8122 - val_loss: 0.4029 - val_accuracy: 0.8229\n",
      "Epoch 384/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3929 - accuracy: 0.8171 - val_loss: 0.4018 - val_accuracy: 0.8194\n",
      "Epoch 385/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3982 - accuracy: 0.8130 - val_loss: 0.4017 - val_accuracy: 0.8274\n",
      "Epoch 386/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3976 - accuracy: 0.8091 - val_loss: 0.4069 - val_accuracy: 0.8160\n",
      "Epoch 387/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3941 - accuracy: 0.8156 - val_loss: 0.4019 - val_accuracy: 0.8160\n",
      "Epoch 388/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3924 - accuracy: 0.8160 - val_loss: 0.4072 - val_accuracy: 0.8149\n",
      "Epoch 389/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3930 - accuracy: 0.8133 - val_loss: 0.4006 - val_accuracy: 0.8206\n",
      "Epoch 390/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3939 - accuracy: 0.8179 - val_loss: 0.4017 - val_accuracy: 0.8229\n",
      "Epoch 391/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3921 - accuracy: 0.8160 - val_loss: 0.4035 - val_accuracy: 0.8206\n",
      "Epoch 392/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3891 - accuracy: 0.8149 - val_loss: 0.4023 - val_accuracy: 0.8206\n",
      "Epoch 393/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3943 - accuracy: 0.8171 - val_loss: 0.4039 - val_accuracy: 0.8229\n",
      "Epoch 394/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3925 - accuracy: 0.8156 - val_loss: 0.4023 - val_accuracy: 0.8274\n",
      "Epoch 395/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3892 - accuracy: 0.8145 - val_loss: 0.4036 - val_accuracy: 0.8206\n",
      "Epoch 396/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3906 - accuracy: 0.8145 - val_loss: 0.4013 - val_accuracy: 0.8229\n",
      "Epoch 397/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3922 - accuracy: 0.8133 - val_loss: 0.4010 - val_accuracy: 0.8286\n",
      "Epoch 398/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3943 - accuracy: 0.8122 - val_loss: 0.4030 - val_accuracy: 0.8229\n",
      "Epoch 399/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3936 - accuracy: 0.8133 - val_loss: 0.3999 - val_accuracy: 0.8229\n",
      "Epoch 400/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3924 - accuracy: 0.8152 - val_loss: 0.4017 - val_accuracy: 0.8263\n",
      "Epoch 401/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3922 - accuracy: 0.8175 - val_loss: 0.4001 - val_accuracy: 0.8229\n",
      "Epoch 402/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3903 - accuracy: 0.8164 - val_loss: 0.4044 - val_accuracy: 0.8251\n",
      "Epoch 403/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3944 - accuracy: 0.8133 - val_loss: 0.4010 - val_accuracy: 0.8229\n",
      "Epoch 404/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3917 - accuracy: 0.8149 - val_loss: 0.4010 - val_accuracy: 0.8240\n",
      "Epoch 405/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3893 - accuracy: 0.8160 - val_loss: 0.3999 - val_accuracy: 0.8263\n",
      "Epoch 406/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3986 - accuracy: 0.8179 - val_loss: 0.4052 - val_accuracy: 0.8171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 407/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3955 - accuracy: 0.8141 - val_loss: 0.4013 - val_accuracy: 0.8263\n",
      "Epoch 408/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3867 - accuracy: 0.8198 - val_loss: 0.4011 - val_accuracy: 0.8263\n",
      "Epoch 409/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3922 - accuracy: 0.8137 - val_loss: 0.4040 - val_accuracy: 0.8194\n",
      "Epoch 410/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3897 - accuracy: 0.8187 - val_loss: 0.4008 - val_accuracy: 0.8183\n",
      "Epoch 411/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3947 - accuracy: 0.8198 - val_loss: 0.4009 - val_accuracy: 0.8240\n",
      "Epoch 412/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3892 - accuracy: 0.8194 - val_loss: 0.4004 - val_accuracy: 0.8251\n",
      "Epoch 413/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3888 - accuracy: 0.8206 - val_loss: 0.4016 - val_accuracy: 0.8240\n",
      "Epoch 414/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3903 - accuracy: 0.8198 - val_loss: 0.4004 - val_accuracy: 0.8263\n",
      "Epoch 415/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3890 - accuracy: 0.8183 - val_loss: 0.3995 - val_accuracy: 0.8229\n",
      "Epoch 416/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3885 - accuracy: 0.8145 - val_loss: 0.4001 - val_accuracy: 0.8217\n",
      "Epoch 417/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3894 - accuracy: 0.8118 - val_loss: 0.4008 - val_accuracy: 0.8217\n",
      "Epoch 418/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3938 - accuracy: 0.8141 - val_loss: 0.4019 - val_accuracy: 0.8274\n",
      "Epoch 419/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3877 - accuracy: 0.8160 - val_loss: 0.3999 - val_accuracy: 0.8240\n",
      "Epoch 420/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3938 - accuracy: 0.8164 - val_loss: 0.3993 - val_accuracy: 0.8251\n",
      "Epoch 421/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3891 - accuracy: 0.8156 - val_loss: 0.4012 - val_accuracy: 0.8274\n",
      "Epoch 422/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3888 - accuracy: 0.8137 - val_loss: 0.4013 - val_accuracy: 0.8274\n",
      "Epoch 423/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3937 - accuracy: 0.8133 - val_loss: 0.3988 - val_accuracy: 0.8251\n",
      "Epoch 424/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3900 - accuracy: 0.8133 - val_loss: 0.3985 - val_accuracy: 0.8229\n",
      "Epoch 425/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3918 - accuracy: 0.8171 - val_loss: 0.4019 - val_accuracy: 0.8194\n",
      "Epoch 426/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3926 - accuracy: 0.8168 - val_loss: 0.3978 - val_accuracy: 0.8263\n",
      "Epoch 427/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3862 - accuracy: 0.8179 - val_loss: 0.4041 - val_accuracy: 0.8286\n",
      "Epoch 428/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3873 - accuracy: 0.8126 - val_loss: 0.3997 - val_accuracy: 0.8251\n",
      "Epoch 429/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3853 - accuracy: 0.8171 - val_loss: 0.4003 - val_accuracy: 0.8274\n",
      "Epoch 430/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3870 - accuracy: 0.8198 - val_loss: 0.4007 - val_accuracy: 0.8240\n",
      "Epoch 431/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3872 - accuracy: 0.8187 - val_loss: 0.3998 - val_accuracy: 0.8240\n",
      "Epoch 432/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3870 - accuracy: 0.8126 - val_loss: 0.3978 - val_accuracy: 0.8263\n",
      "Epoch 433/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3899 - accuracy: 0.8149 - val_loss: 0.3976 - val_accuracy: 0.8263\n",
      "Epoch 434/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3897 - accuracy: 0.8133 - val_loss: 0.3982 - val_accuracy: 0.8263\n",
      "Epoch 435/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3899 - accuracy: 0.8194 - val_loss: 0.4052 - val_accuracy: 0.8194\n",
      "Epoch 436/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3870 - accuracy: 0.8156 - val_loss: 0.3994 - val_accuracy: 0.8274\n",
      "Epoch 437/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3858 - accuracy: 0.8171 - val_loss: 0.4007 - val_accuracy: 0.8274\n",
      "Epoch 438/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3874 - accuracy: 0.8198 - val_loss: 0.3973 - val_accuracy: 0.8229\n",
      "Epoch 439/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3877 - accuracy: 0.8168 - val_loss: 0.3976 - val_accuracy: 0.8240\n",
      "Epoch 440/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3860 - accuracy: 0.8194 - val_loss: 0.3980 - val_accuracy: 0.8251\n",
      "Epoch 441/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3884 - accuracy: 0.8183 - val_loss: 0.3973 - val_accuracy: 0.8320\n",
      "Epoch 442/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3863 - accuracy: 0.8229 - val_loss: 0.4002 - val_accuracy: 0.8251\n",
      "Epoch 443/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3877 - accuracy: 0.8137 - val_loss: 0.3978 - val_accuracy: 0.8263\n",
      "Epoch 444/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3881 - accuracy: 0.8145 - val_loss: 0.4035 - val_accuracy: 0.8274\n",
      "Epoch 445/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3910 - accuracy: 0.8152 - val_loss: 0.4008 - val_accuracy: 0.8183\n",
      "Epoch 446/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3939 - accuracy: 0.8141 - val_loss: 0.4002 - val_accuracy: 0.8263\n",
      "Epoch 447/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3895 - accuracy: 0.8179 - val_loss: 0.3985 - val_accuracy: 0.8240\n",
      "Epoch 448/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3829 - accuracy: 0.8225 - val_loss: 0.4005 - val_accuracy: 0.8229\n",
      "Epoch 449/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3869 - accuracy: 0.8194 - val_loss: 0.3983 - val_accuracy: 0.8229\n",
      "Epoch 450/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3884 - accuracy: 0.8194 - val_loss: 0.3996 - val_accuracy: 0.8297\n",
      "Epoch 451/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3860 - accuracy: 0.8194 - val_loss: 0.3976 - val_accuracy: 0.8240\n",
      "Epoch 452/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3966 - accuracy: 0.8091 - val_loss: 0.3982 - val_accuracy: 0.8251\n",
      "Epoch 453/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3892 - accuracy: 0.8122 - val_loss: 0.3996 - val_accuracy: 0.8286\n",
      "Epoch 454/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3869 - accuracy: 0.8190 - val_loss: 0.3977 - val_accuracy: 0.8194\n",
      "Epoch 455/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3892 - accuracy: 0.8171 - val_loss: 0.4012 - val_accuracy: 0.8251\n",
      "Epoch 456/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3911 - accuracy: 0.8179 - val_loss: 0.3980 - val_accuracy: 0.8229\n",
      "Epoch 457/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3880 - accuracy: 0.8160 - val_loss: 0.4001 - val_accuracy: 0.8320\n",
      "Epoch 458/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3896 - accuracy: 0.8160 - val_loss: 0.3995 - val_accuracy: 0.8217\n",
      "Epoch 459/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3839 - accuracy: 0.8217 - val_loss: 0.3967 - val_accuracy: 0.8263\n",
      "Epoch 460/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3878 - accuracy: 0.8156 - val_loss: 0.3977 - val_accuracy: 0.8320\n",
      "Epoch 461/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3873 - accuracy: 0.8171 - val_loss: 0.3971 - val_accuracy: 0.8274\n",
      "Epoch 462/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3879 - accuracy: 0.8141 - val_loss: 0.3988 - val_accuracy: 0.8286\n",
      "Epoch 463/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3870 - accuracy: 0.8156 - val_loss: 0.3995 - val_accuracy: 0.8274\n",
      "Epoch 464/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3852 - accuracy: 0.8190 - val_loss: 0.3983 - val_accuracy: 0.8217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 465/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3896 - accuracy: 0.8149 - val_loss: 0.4026 - val_accuracy: 0.8263\n",
      "Epoch 466/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4012 - accuracy: 0.8091 - val_loss: 0.3997 - val_accuracy: 0.8331\n",
      "Epoch 467/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3916 - accuracy: 0.8122 - val_loss: 0.3964 - val_accuracy: 0.8263\n",
      "Epoch 468/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3924 - accuracy: 0.8156 - val_loss: 0.4049 - val_accuracy: 0.8183\n",
      "Epoch 469/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3849 - accuracy: 0.8202 - val_loss: 0.4002 - val_accuracy: 0.8171\n",
      "Epoch 470/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3894 - accuracy: 0.8183 - val_loss: 0.4040 - val_accuracy: 0.8240\n",
      "Epoch 471/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3883 - accuracy: 0.8190 - val_loss: 0.3977 - val_accuracy: 0.8229\n",
      "Epoch 472/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3860 - accuracy: 0.8202 - val_loss: 0.3997 - val_accuracy: 0.8309\n",
      "Epoch 473/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3880 - accuracy: 0.8152 - val_loss: 0.3980 - val_accuracy: 0.8263\n",
      "Epoch 474/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3806 - accuracy: 0.8236 - val_loss: 0.3978 - val_accuracy: 0.8251\n",
      "Epoch 475/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3821 - accuracy: 0.8210 - val_loss: 0.3996 - val_accuracy: 0.8309\n",
      "Epoch 476/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3791 - accuracy: 0.8210 - val_loss: 0.3988 - val_accuracy: 0.8251\n",
      "Epoch 477/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3852 - accuracy: 0.8190 - val_loss: 0.3987 - val_accuracy: 0.8251\n",
      "Epoch 478/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3831 - accuracy: 0.8183 - val_loss: 0.3970 - val_accuracy: 0.8297\n",
      "Epoch 479/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3845 - accuracy: 0.8145 - val_loss: 0.3978 - val_accuracy: 0.8309\n",
      "Epoch 480/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3859 - accuracy: 0.8206 - val_loss: 0.3972 - val_accuracy: 0.8240\n",
      "Epoch 481/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3869 - accuracy: 0.8171 - val_loss: 0.3992 - val_accuracy: 0.8217\n",
      "Epoch 482/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3821 - accuracy: 0.8190 - val_loss: 0.3981 - val_accuracy: 0.8274\n",
      "Epoch 483/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3792 - accuracy: 0.8232 - val_loss: 0.4004 - val_accuracy: 0.8286\n",
      "Epoch 484/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3835 - accuracy: 0.8213 - val_loss: 0.3982 - val_accuracy: 0.8263\n",
      "Epoch 485/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3850 - accuracy: 0.8187 - val_loss: 0.3982 - val_accuracy: 0.8286\n",
      "Epoch 486/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3837 - accuracy: 0.8156 - val_loss: 0.4007 - val_accuracy: 0.8251\n",
      "Epoch 487/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3825 - accuracy: 0.8244 - val_loss: 0.3979 - val_accuracy: 0.8240\n",
      "Epoch 488/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3810 - accuracy: 0.8221 - val_loss: 0.3967 - val_accuracy: 0.8320\n",
      "Epoch 489/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3817 - accuracy: 0.8164 - val_loss: 0.3979 - val_accuracy: 0.8217\n",
      "Epoch 490/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3824 - accuracy: 0.8171 - val_loss: 0.3993 - val_accuracy: 0.8286\n",
      "Epoch 491/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3849 - accuracy: 0.8217 - val_loss: 0.3981 - val_accuracy: 0.8274\n",
      "Epoch 492/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3849 - accuracy: 0.8198 - val_loss: 0.3973 - val_accuracy: 0.8206\n",
      "Epoch 493/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3827 - accuracy: 0.8175 - val_loss: 0.3984 - val_accuracy: 0.8206\n",
      "Epoch 494/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3814 - accuracy: 0.8187 - val_loss: 0.3997 - val_accuracy: 0.8240\n",
      "Epoch 495/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3837 - accuracy: 0.8221 - val_loss: 0.4022 - val_accuracy: 0.8183\n",
      "Epoch 496/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3859 - accuracy: 0.8145 - val_loss: 0.3987 - val_accuracy: 0.8354\n",
      "Epoch 497/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3838 - accuracy: 0.8164 - val_loss: 0.3966 - val_accuracy: 0.8251\n",
      "Epoch 498/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3807 - accuracy: 0.8164 - val_loss: 0.3980 - val_accuracy: 0.8240\n",
      "Epoch 499/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3845 - accuracy: 0.8179 - val_loss: 0.3981 - val_accuracy: 0.8240\n",
      "Epoch 500/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3808 - accuracy: 0.8213 - val_loss: 0.3970 - val_accuracy: 0.8263\n",
      "Epoch 501/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3835 - accuracy: 0.8229 - val_loss: 0.3969 - val_accuracy: 0.8263\n",
      "Epoch 502/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3827 - accuracy: 0.8221 - val_loss: 0.3973 - val_accuracy: 0.8263\n",
      "Epoch 503/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3830 - accuracy: 0.8221 - val_loss: 0.3984 - val_accuracy: 0.8320\n",
      "Epoch 504/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3834 - accuracy: 0.8190 - val_loss: 0.3961 - val_accuracy: 0.8217\n",
      "Epoch 505/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3843 - accuracy: 0.8160 - val_loss: 0.3979 - val_accuracy: 0.8263\n",
      "Epoch 506/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3777 - accuracy: 0.8194 - val_loss: 0.3991 - val_accuracy: 0.8286\n",
      "Epoch 507/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3862 - accuracy: 0.8194 - val_loss: 0.3980 - val_accuracy: 0.8286\n",
      "Epoch 508/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3851 - accuracy: 0.8156 - val_loss: 0.3988 - val_accuracy: 0.8286\n",
      "Epoch 509/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3852 - accuracy: 0.8175 - val_loss: 0.4000 - val_accuracy: 0.8240\n",
      "Epoch 510/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3814 - accuracy: 0.8156 - val_loss: 0.3974 - val_accuracy: 0.8251\n",
      "Epoch 511/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3835 - accuracy: 0.8206 - val_loss: 0.3961 - val_accuracy: 0.8263\n",
      "Epoch 512/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3827 - accuracy: 0.8206 - val_loss: 0.3971 - val_accuracy: 0.8286\n",
      "Epoch 513/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3785 - accuracy: 0.8236 - val_loss: 0.3969 - val_accuracy: 0.8263\n",
      "Epoch 514/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3844 - accuracy: 0.8175 - val_loss: 0.3952 - val_accuracy: 0.8263\n",
      "Epoch 515/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3823 - accuracy: 0.8175 - val_loss: 0.4008 - val_accuracy: 0.8286\n",
      "Epoch 516/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3852 - accuracy: 0.8168 - val_loss: 0.3974 - val_accuracy: 0.8331\n",
      "Epoch 517/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3810 - accuracy: 0.8213 - val_loss: 0.3945 - val_accuracy: 0.8263\n",
      "Epoch 518/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3870 - accuracy: 0.8171 - val_loss: 0.3985 - val_accuracy: 0.8297\n",
      "Epoch 519/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3800 - accuracy: 0.8183 - val_loss: 0.3972 - val_accuracy: 0.8229\n",
      "Epoch 520/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3786 - accuracy: 0.8198 - val_loss: 0.3971 - val_accuracy: 0.8274\n",
      "Epoch 521/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3824 - accuracy: 0.8198 - val_loss: 0.3985 - val_accuracy: 0.8274\n",
      "Epoch 522/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3815 - accuracy: 0.8217 - val_loss: 0.3959 - val_accuracy: 0.8263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 523/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3819 - accuracy: 0.8202 - val_loss: 0.3996 - val_accuracy: 0.8331\n",
      "Epoch 524/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3890 - accuracy: 0.8149 - val_loss: 0.3967 - val_accuracy: 0.8206\n",
      "Epoch 525/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3834 - accuracy: 0.8213 - val_loss: 0.3981 - val_accuracy: 0.8309\n",
      "Epoch 526/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3771 - accuracy: 0.8213 - val_loss: 0.3982 - val_accuracy: 0.8309\n",
      "Epoch 527/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3790 - accuracy: 0.8198 - val_loss: 0.3962 - val_accuracy: 0.8251\n",
      "Epoch 528/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3808 - accuracy: 0.8213 - val_loss: 0.3986 - val_accuracy: 0.8240\n",
      "Epoch 529/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3828 - accuracy: 0.8229 - val_loss: 0.3964 - val_accuracy: 0.8251\n",
      "Epoch 530/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3832 - accuracy: 0.8168 - val_loss: 0.3992 - val_accuracy: 0.8286\n",
      "Epoch 531/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3814 - accuracy: 0.8202 - val_loss: 0.3958 - val_accuracy: 0.8286\n",
      "Epoch 532/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3852 - accuracy: 0.8179 - val_loss: 0.3942 - val_accuracy: 0.8263\n",
      "Epoch 533/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3803 - accuracy: 0.8217 - val_loss: 0.4008 - val_accuracy: 0.8137\n",
      "Epoch 534/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3821 - accuracy: 0.8168 - val_loss: 0.3972 - val_accuracy: 0.8229\n",
      "Epoch 535/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3838 - accuracy: 0.8202 - val_loss: 0.3971 - val_accuracy: 0.8251\n",
      "Epoch 536/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3786 - accuracy: 0.8217 - val_loss: 0.3961 - val_accuracy: 0.8286\n",
      "Epoch 537/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3758 - accuracy: 0.8175 - val_loss: 0.3974 - val_accuracy: 0.8297\n",
      "Epoch 538/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3797 - accuracy: 0.8202 - val_loss: 0.3978 - val_accuracy: 0.8251\n",
      "Epoch 539/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3794 - accuracy: 0.8210 - val_loss: 0.3995 - val_accuracy: 0.8297\n",
      "Epoch 540/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3807 - accuracy: 0.8240 - val_loss: 0.3979 - val_accuracy: 0.8251\n",
      "Epoch 541/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3803 - accuracy: 0.8248 - val_loss: 0.3965 - val_accuracy: 0.8263\n",
      "Epoch 542/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3776 - accuracy: 0.8225 - val_loss: 0.3956 - val_accuracy: 0.8263\n",
      "Epoch 543/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3828 - accuracy: 0.8175 - val_loss: 0.3943 - val_accuracy: 0.8240\n",
      "Epoch 544/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3845 - accuracy: 0.8206 - val_loss: 0.3981 - val_accuracy: 0.8263\n",
      "Epoch 545/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3821 - accuracy: 0.8210 - val_loss: 0.3945 - val_accuracy: 0.8206\n",
      "Epoch 546/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3938 - accuracy: 0.8118 - val_loss: 0.3963 - val_accuracy: 0.8309\n",
      "Epoch 547/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3932 - accuracy: 0.8099 - val_loss: 0.3958 - val_accuracy: 0.8320\n",
      "Epoch 548/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3840 - accuracy: 0.8202 - val_loss: 0.3933 - val_accuracy: 0.8194\n",
      "Epoch 549/1000\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3776 - accuracy: 0.83 - 0s 4ms/step - loss: 0.3836 - accuracy: 0.8190 - val_loss: 0.4019 - val_accuracy: 0.8183\n",
      "Epoch 550/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3780 - accuracy: 0.8232 - val_loss: 0.3934 - val_accuracy: 0.8217\n",
      "Epoch 551/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3818 - accuracy: 0.8190 - val_loss: 0.4009 - val_accuracy: 0.8240\n",
      "Epoch 552/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3815 - accuracy: 0.8198 - val_loss: 0.3963 - val_accuracy: 0.8240\n",
      "Epoch 553/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3771 - accuracy: 0.8217 - val_loss: 0.4010 - val_accuracy: 0.8263\n",
      "Epoch 554/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3832 - accuracy: 0.8236 - val_loss: 0.3978 - val_accuracy: 0.8274\n",
      "Epoch 555/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3813 - accuracy: 0.8164 - val_loss: 0.3967 - val_accuracy: 0.8240\n",
      "Epoch 556/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3782 - accuracy: 0.8259 - val_loss: 0.3986 - val_accuracy: 0.8320\n",
      "Epoch 557/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3777 - accuracy: 0.8236 - val_loss: 0.3972 - val_accuracy: 0.8194\n",
      "Epoch 558/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3764 - accuracy: 0.8236 - val_loss: 0.3997 - val_accuracy: 0.8297\n",
      "Epoch 559/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3769 - accuracy: 0.8202 - val_loss: 0.3970 - val_accuracy: 0.8229\n",
      "Epoch 560/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3820 - accuracy: 0.8202 - val_loss: 0.4028 - val_accuracy: 0.8194\n",
      "Epoch 561/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3860 - accuracy: 0.8122 - val_loss: 0.4019 - val_accuracy: 0.8171\n",
      "Epoch 562/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3780 - accuracy: 0.8213 - val_loss: 0.3992 - val_accuracy: 0.8240\n",
      "Epoch 563/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3858 - accuracy: 0.8202 - val_loss: 0.3979 - val_accuracy: 0.8263\n",
      "Epoch 564/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3775 - accuracy: 0.8210 - val_loss: 0.3994 - val_accuracy: 0.8263\n",
      "Epoch 565/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3803 - accuracy: 0.8198 - val_loss: 0.3987 - val_accuracy: 0.8309\n",
      "Epoch 566/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3788 - accuracy: 0.8213 - val_loss: 0.3967 - val_accuracy: 0.8297\n",
      "Epoch 567/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3776 - accuracy: 0.8198 - val_loss: 0.3960 - val_accuracy: 0.8229\n",
      "Epoch 568/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3791 - accuracy: 0.8202 - val_loss: 0.3966 - val_accuracy: 0.8309\n",
      "Epoch 569/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3807 - accuracy: 0.8217 - val_loss: 0.3952 - val_accuracy: 0.8274\n",
      "Epoch 570/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3731 - accuracy: 0.8278 - val_loss: 0.3960 - val_accuracy: 0.8309\n",
      "Epoch 571/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3759 - accuracy: 0.8206 - val_loss: 0.3942 - val_accuracy: 0.8206\n",
      "Epoch 572/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3799 - accuracy: 0.8229 - val_loss: 0.3936 - val_accuracy: 0.8229\n",
      "Epoch 573/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3746 - accuracy: 0.8240 - val_loss: 0.3942 - val_accuracy: 0.8297\n",
      "Epoch 574/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3778 - accuracy: 0.8175 - val_loss: 0.3942 - val_accuracy: 0.8229\n",
      "Epoch 575/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3735 - accuracy: 0.8225 - val_loss: 0.3972 - val_accuracy: 0.8263\n",
      "Epoch 576/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3772 - accuracy: 0.8187 - val_loss: 0.3966 - val_accuracy: 0.8251\n",
      "Epoch 577/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3777 - accuracy: 0.8198 - val_loss: 0.3955 - val_accuracy: 0.8274\n",
      "Epoch 578/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3803 - accuracy: 0.8194 - val_loss: 0.3954 - val_accuracy: 0.8194\n",
      "Epoch 579/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3791 - accuracy: 0.8179 - val_loss: 0.3954 - val_accuracy: 0.8320\n",
      "Epoch 580/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3809 - accuracy: 0.8145 - val_loss: 0.3937 - val_accuracy: 0.8251\n",
      "Epoch 581/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3785 - accuracy: 0.8202 - val_loss: 0.3958 - val_accuracy: 0.8274\n",
      "Epoch 582/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3806 - accuracy: 0.8194 - val_loss: 0.3929 - val_accuracy: 0.8206\n",
      "Epoch 583/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3759 - accuracy: 0.8232 - val_loss: 0.3978 - val_accuracy: 0.8240\n",
      "Epoch 584/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3777 - accuracy: 0.8183 - val_loss: 0.3935 - val_accuracy: 0.8240\n",
      "Epoch 585/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3852 - accuracy: 0.8183 - val_loss: 0.3977 - val_accuracy: 0.8274\n",
      "Epoch 586/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3788 - accuracy: 0.8198 - val_loss: 0.3963 - val_accuracy: 0.8217\n",
      "Epoch 587/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3841 - accuracy: 0.8187 - val_loss: 0.4002 - val_accuracy: 0.8194\n",
      "Epoch 588/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3774 - accuracy: 0.8232 - val_loss: 0.3965 - val_accuracy: 0.8286\n",
      "Epoch 589/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3771 - accuracy: 0.8225 - val_loss: 0.3971 - val_accuracy: 0.8251\n",
      "Epoch 590/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3771 - accuracy: 0.8187 - val_loss: 0.3956 - val_accuracy: 0.8274\n",
      "Epoch 591/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3789 - accuracy: 0.8221 - val_loss: 0.4012 - val_accuracy: 0.8194\n",
      "Epoch 592/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3790 - accuracy: 0.8248 - val_loss: 0.3967 - val_accuracy: 0.8251\n",
      "Epoch 593/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3767 - accuracy: 0.8210 - val_loss: 0.3973 - val_accuracy: 0.8286\n",
      "Epoch 594/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3768 - accuracy: 0.8183 - val_loss: 0.3954 - val_accuracy: 0.8229\n",
      "Epoch 595/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3760 - accuracy: 0.8221 - val_loss: 0.4007 - val_accuracy: 0.8240\n",
      "Epoch 596/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3773 - accuracy: 0.8168 - val_loss: 0.3961 - val_accuracy: 0.8263\n",
      "Epoch 597/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3766 - accuracy: 0.8217 - val_loss: 0.3973 - val_accuracy: 0.8240\n",
      "Epoch 598/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3789 - accuracy: 0.8263 - val_loss: 0.3981 - val_accuracy: 0.8263\n",
      "Epoch 599/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3776 - accuracy: 0.8190 - val_loss: 0.3954 - val_accuracy: 0.8309\n",
      "Epoch 600/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3759 - accuracy: 0.8190 - val_loss: 0.3936 - val_accuracy: 0.8183\n",
      "Epoch 601/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3791 - accuracy: 0.8190 - val_loss: 0.3944 - val_accuracy: 0.8251\n",
      "Epoch 602/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3787 - accuracy: 0.8221 - val_loss: 0.3964 - val_accuracy: 0.8263\n",
      "Epoch 603/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3794 - accuracy: 0.8187 - val_loss: 0.3968 - val_accuracy: 0.8286\n",
      "Epoch 604/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3757 - accuracy: 0.8210 - val_loss: 0.3964 - val_accuracy: 0.8263\n",
      "Epoch 605/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3750 - accuracy: 0.8190 - val_loss: 0.3956 - val_accuracy: 0.8251\n",
      "Epoch 606/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3774 - accuracy: 0.8263 - val_loss: 0.3980 - val_accuracy: 0.8206\n",
      "Epoch 607/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3764 - accuracy: 0.8229 - val_loss: 0.3933 - val_accuracy: 0.8263\n",
      "Epoch 608/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3791 - accuracy: 0.8168 - val_loss: 0.3990 - val_accuracy: 0.8309\n",
      "Epoch 609/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3820 - accuracy: 0.8217 - val_loss: 0.3925 - val_accuracy: 0.8217\n",
      "Epoch 610/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3855 - accuracy: 0.8156 - val_loss: 0.3981 - val_accuracy: 0.8229\n",
      "Epoch 611/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3796 - accuracy: 0.8244 - val_loss: 0.3965 - val_accuracy: 0.8251\n",
      "Epoch 612/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3721 - accuracy: 0.8263 - val_loss: 0.3936 - val_accuracy: 0.8183\n",
      "Epoch 613/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3763 - accuracy: 0.8255 - val_loss: 0.3932 - val_accuracy: 0.8217\n",
      "Epoch 614/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3743 - accuracy: 0.8175 - val_loss: 0.3923 - val_accuracy: 0.8194\n",
      "Epoch 615/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3775 - accuracy: 0.8213 - val_loss: 0.3957 - val_accuracy: 0.8251\n",
      "Epoch 616/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3791 - accuracy: 0.8236 - val_loss: 0.3931 - val_accuracy: 0.8251\n",
      "Epoch 617/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3770 - accuracy: 0.8190 - val_loss: 0.3940 - val_accuracy: 0.8251\n",
      "Epoch 618/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3763 - accuracy: 0.8194 - val_loss: 0.3976 - val_accuracy: 0.8229\n",
      "Epoch 619/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3766 - accuracy: 0.8232 - val_loss: 0.3948 - val_accuracy: 0.8206\n",
      "Epoch 620/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3725 - accuracy: 0.8244 - val_loss: 0.4002 - val_accuracy: 0.8183\n",
      "Epoch 621/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3761 - accuracy: 0.8240 - val_loss: 0.3962 - val_accuracy: 0.8206\n",
      "Epoch 622/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3767 - accuracy: 0.8202 - val_loss: 0.3978 - val_accuracy: 0.8251\n",
      "Epoch 623/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3756 - accuracy: 0.8187 - val_loss: 0.3947 - val_accuracy: 0.8263\n",
      "Epoch 624/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3778 - accuracy: 0.8168 - val_loss: 0.3980 - val_accuracy: 0.8274\n",
      "Epoch 625/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3739 - accuracy: 0.8229 - val_loss: 0.3940 - val_accuracy: 0.8206\n",
      "Epoch 626/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3717 - accuracy: 0.8267 - val_loss: 0.3977 - val_accuracy: 0.8263\n",
      "Epoch 627/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3758 - accuracy: 0.8232 - val_loss: 0.3950 - val_accuracy: 0.8206\n",
      "Epoch 628/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3802 - accuracy: 0.8164 - val_loss: 0.3951 - val_accuracy: 0.8274\n",
      "Epoch 629/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3729 - accuracy: 0.8232 - val_loss: 0.3960 - val_accuracy: 0.8217\n",
      "Epoch 630/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3720 - accuracy: 0.8187 - val_loss: 0.3952 - val_accuracy: 0.8229\n",
      "Epoch 631/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3745 - accuracy: 0.8251 - val_loss: 0.3957 - val_accuracy: 0.8251\n",
      "Epoch 632/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3740 - accuracy: 0.8244 - val_loss: 0.3973 - val_accuracy: 0.8183\n",
      "Epoch 633/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3791 - accuracy: 0.8225 - val_loss: 0.4006 - val_accuracy: 0.8217\n",
      "Epoch 634/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3741 - accuracy: 0.8236 - val_loss: 0.3961 - val_accuracy: 0.8286\n",
      "Epoch 635/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3763 - accuracy: 0.8183 - val_loss: 0.3953 - val_accuracy: 0.8251\n",
      "Epoch 636/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3751 - accuracy: 0.8206 - val_loss: 0.3954 - val_accuracy: 0.8286\n",
      "Epoch 637/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3771 - accuracy: 0.8210 - val_loss: 0.3968 - val_accuracy: 0.8263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 638/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3842 - accuracy: 0.8202 - val_loss: 0.3960 - val_accuracy: 0.8274\n",
      "Epoch 639/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3805 - accuracy: 0.8198 - val_loss: 0.3953 - val_accuracy: 0.8251\n",
      "Epoch 640/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3783 - accuracy: 0.8126 - val_loss: 0.3953 - val_accuracy: 0.8251\n",
      "Epoch 641/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3792 - accuracy: 0.8210 - val_loss: 0.3931 - val_accuracy: 0.8229\n",
      "Epoch 642/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3780 - accuracy: 0.8187 - val_loss: 0.3974 - val_accuracy: 0.8217\n",
      "Epoch 643/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3761 - accuracy: 0.8267 - val_loss: 0.3925 - val_accuracy: 0.8274\n",
      "Epoch 644/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3746 - accuracy: 0.8221 - val_loss: 0.3971 - val_accuracy: 0.8251\n",
      "Epoch 645/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3762 - accuracy: 0.8244 - val_loss: 0.3951 - val_accuracy: 0.8217\n",
      "Epoch 646/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3764 - accuracy: 0.8225 - val_loss: 0.3976 - val_accuracy: 0.8240\n",
      "Epoch 647/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3749 - accuracy: 0.8194 - val_loss: 0.3947 - val_accuracy: 0.8240\n",
      "Epoch 648/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3724 - accuracy: 0.8232 - val_loss: 0.3969 - val_accuracy: 0.8251\n",
      "Epoch 649/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3780 - accuracy: 0.8225 - val_loss: 0.3954 - val_accuracy: 0.8194\n",
      "Epoch 650/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3798 - accuracy: 0.8194 - val_loss: 0.3965 - val_accuracy: 0.8263\n",
      "Epoch 651/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3787 - accuracy: 0.8248 - val_loss: 0.3947 - val_accuracy: 0.8274\n",
      "Epoch 652/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3757 - accuracy: 0.8198 - val_loss: 0.3949 - val_accuracy: 0.8251\n",
      "Epoch 653/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3769 - accuracy: 0.8194 - val_loss: 0.3996 - val_accuracy: 0.8240\n",
      "Epoch 654/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3790 - accuracy: 0.8229 - val_loss: 0.3943 - val_accuracy: 0.8240\n",
      "Epoch 655/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3777 - accuracy: 0.8263 - val_loss: 0.4019 - val_accuracy: 0.8274\n",
      "Epoch 656/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3762 - accuracy: 0.8221 - val_loss: 0.3936 - val_accuracy: 0.8217\n",
      "Epoch 657/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3727 - accuracy: 0.8198 - val_loss: 0.3956 - val_accuracy: 0.8263\n",
      "Epoch 658/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3737 - accuracy: 0.8263 - val_loss: 0.3932 - val_accuracy: 0.8206\n",
      "Epoch 659/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3722 - accuracy: 0.8286 - val_loss: 0.3953 - val_accuracy: 0.8274\n",
      "Epoch 660/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3724 - accuracy: 0.8194 - val_loss: 0.3980 - val_accuracy: 0.8286\n",
      "Epoch 661/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3756 - accuracy: 0.8236 - val_loss: 0.3949 - val_accuracy: 0.8240\n",
      "Epoch 662/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3724 - accuracy: 0.8232 - val_loss: 0.3926 - val_accuracy: 0.8229\n",
      "Epoch 663/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3743 - accuracy: 0.8255 - val_loss: 0.3955 - val_accuracy: 0.8274\n",
      "Epoch 664/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3720 - accuracy: 0.8270 - val_loss: 0.3924 - val_accuracy: 0.8274\n",
      "Epoch 665/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3722 - accuracy: 0.8225 - val_loss: 0.3937 - val_accuracy: 0.8251\n",
      "Epoch 666/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3720 - accuracy: 0.8217 - val_loss: 0.3949 - val_accuracy: 0.8263\n",
      "Epoch 667/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3752 - accuracy: 0.8210 - val_loss: 0.3995 - val_accuracy: 0.8263\n",
      "Epoch 668/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3703 - accuracy: 0.8278 - val_loss: 0.3961 - val_accuracy: 0.8229\n",
      "Epoch 669/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3746 - accuracy: 0.8217 - val_loss: 0.3995 - val_accuracy: 0.8274\n",
      "Epoch 670/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3763 - accuracy: 0.8274 - val_loss: 0.3958 - val_accuracy: 0.8274\n",
      "Epoch 671/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3659 - accuracy: 0.8236 - val_loss: 0.3941 - val_accuracy: 0.8183\n",
      "Epoch 672/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3698 - accuracy: 0.8251 - val_loss: 0.3945 - val_accuracy: 0.8331\n",
      "Epoch 673/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3730 - accuracy: 0.8255 - val_loss: 0.3933 - val_accuracy: 0.8286\n",
      "Epoch 674/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3711 - accuracy: 0.8240 - val_loss: 0.3963 - val_accuracy: 0.8240\n",
      "Epoch 675/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3707 - accuracy: 0.8270 - val_loss: 0.3951 - val_accuracy: 0.8183\n",
      "Epoch 676/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3716 - accuracy: 0.8248 - val_loss: 0.3940 - val_accuracy: 0.8240\n",
      "Epoch 677/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3752 - accuracy: 0.8206 - val_loss: 0.3956 - val_accuracy: 0.8320\n",
      "Epoch 678/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3815 - accuracy: 0.8206 - val_loss: 0.3989 - val_accuracy: 0.8286\n",
      "Epoch 679/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3792 - accuracy: 0.8213 - val_loss: 0.3941 - val_accuracy: 0.8229\n",
      "Epoch 680/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3773 - accuracy: 0.8225 - val_loss: 0.4019 - val_accuracy: 0.8183\n",
      "Epoch 681/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3758 - accuracy: 0.8270 - val_loss: 0.3929 - val_accuracy: 0.8217\n",
      "Epoch 682/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3714 - accuracy: 0.8190 - val_loss: 0.4027 - val_accuracy: 0.8171\n",
      "Epoch 683/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3720 - accuracy: 0.8190 - val_loss: 0.3926 - val_accuracy: 0.8229\n",
      "Epoch 684/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3765 - accuracy: 0.8194 - val_loss: 0.3997 - val_accuracy: 0.8217\n",
      "Epoch 685/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3760 - accuracy: 0.8244 - val_loss: 0.3940 - val_accuracy: 0.8217\n",
      "Epoch 686/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3762 - accuracy: 0.8210 - val_loss: 0.3934 - val_accuracy: 0.8217\n",
      "Epoch 687/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3729 - accuracy: 0.8225 - val_loss: 0.3964 - val_accuracy: 0.8263\n",
      "Epoch 688/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3730 - accuracy: 0.8198 - val_loss: 0.3941 - val_accuracy: 0.8274\n",
      "Epoch 689/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3694 - accuracy: 0.8293 - val_loss: 0.3930 - val_accuracy: 0.8240\n",
      "Epoch 690/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3725 - accuracy: 0.8255 - val_loss: 0.3999 - val_accuracy: 0.8217\n",
      "Epoch 691/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3910 - accuracy: 0.8088 - val_loss: 0.3955 - val_accuracy: 0.8240\n",
      "Epoch 692/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3823 - accuracy: 0.8145 - val_loss: 0.3935 - val_accuracy: 0.8286\n",
      "Epoch 693/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3788 - accuracy: 0.8236 - val_loss: 0.4034 - val_accuracy: 0.8183\n",
      "Epoch 694/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3733 - accuracy: 0.8270 - val_loss: 0.3941 - val_accuracy: 0.8194\n",
      "Epoch 695/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3739 - accuracy: 0.8217 - val_loss: 0.3988 - val_accuracy: 0.8240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 696/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3710 - accuracy: 0.8259 - val_loss: 0.3929 - val_accuracy: 0.8240\n",
      "Epoch 697/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3734 - accuracy: 0.8160 - val_loss: 0.4005 - val_accuracy: 0.8240\n",
      "Epoch 698/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3778 - accuracy: 0.8221 - val_loss: 0.3960 - val_accuracy: 0.8217\n",
      "Epoch 699/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3726 - accuracy: 0.8225 - val_loss: 0.3944 - val_accuracy: 0.8251\n",
      "Epoch 700/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3701 - accuracy: 0.8244 - val_loss: 0.3949 - val_accuracy: 0.8240\n",
      "Epoch 701/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3815 - accuracy: 0.8152 - val_loss: 0.3941 - val_accuracy: 0.8263\n",
      "Epoch 702/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3710 - accuracy: 0.8236 - val_loss: 0.3966 - val_accuracy: 0.8229\n",
      "Epoch 703/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3685 - accuracy: 0.8259 - val_loss: 0.3961 - val_accuracy: 0.8183\n",
      "Epoch 704/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3701 - accuracy: 0.8244 - val_loss: 0.3985 - val_accuracy: 0.8217\n",
      "Epoch 705/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3692 - accuracy: 0.8263 - val_loss: 0.3953 - val_accuracy: 0.8251\n",
      "Epoch 706/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3728 - accuracy: 0.8259 - val_loss: 0.3961 - val_accuracy: 0.8171\n",
      "Epoch 707/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3787 - accuracy: 0.8179 - val_loss: 0.3920 - val_accuracy: 0.8229\n",
      "Epoch 708/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3669 - accuracy: 0.8263 - val_loss: 0.3987 - val_accuracy: 0.8240\n",
      "Epoch 709/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3676 - accuracy: 0.8259 - val_loss: 0.3915 - val_accuracy: 0.8229\n",
      "Epoch 710/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3736 - accuracy: 0.8202 - val_loss: 0.3954 - val_accuracy: 0.8240\n",
      "Epoch 711/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3720 - accuracy: 0.8229 - val_loss: 0.3930 - val_accuracy: 0.8251\n",
      "Epoch 712/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3693 - accuracy: 0.8297 - val_loss: 0.3926 - val_accuracy: 0.8206\n",
      "Epoch 713/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3725 - accuracy: 0.8244 - val_loss: 0.3962 - val_accuracy: 0.8217\n",
      "Epoch 714/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3702 - accuracy: 0.8251 - val_loss: 0.3958 - val_accuracy: 0.8263\n",
      "Epoch 715/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3696 - accuracy: 0.8259 - val_loss: 0.3928 - val_accuracy: 0.8194\n",
      "Epoch 716/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3703 - accuracy: 0.8240 - val_loss: 0.3980 - val_accuracy: 0.8206\n",
      "Epoch 717/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3686 - accuracy: 0.8263 - val_loss: 0.3992 - val_accuracy: 0.8194\n",
      "Epoch 718/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3712 - accuracy: 0.8206 - val_loss: 0.3965 - val_accuracy: 0.8194\n",
      "Epoch 719/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3724 - accuracy: 0.8259 - val_loss: 0.3936 - val_accuracy: 0.8251\n",
      "Epoch 720/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3701 - accuracy: 0.8240 - val_loss: 0.3932 - val_accuracy: 0.8217\n",
      "Epoch 721/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3671 - accuracy: 0.8255 - val_loss: 0.3931 - val_accuracy: 0.8217\n",
      "Epoch 722/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3683 - accuracy: 0.8282 - val_loss: 0.3952 - val_accuracy: 0.8183\n",
      "Epoch 723/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3652 - accuracy: 0.8267 - val_loss: 0.3931 - val_accuracy: 0.8194\n",
      "Epoch 724/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3710 - accuracy: 0.8221 - val_loss: 0.3948 - val_accuracy: 0.8240\n",
      "Epoch 725/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3693 - accuracy: 0.8244 - val_loss: 0.3965 - val_accuracy: 0.8206\n",
      "Epoch 726/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3658 - accuracy: 0.8278 - val_loss: 0.3921 - val_accuracy: 0.8286\n",
      "Epoch 727/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3698 - accuracy: 0.8217 - val_loss: 0.3956 - val_accuracy: 0.8274\n",
      "Epoch 728/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3753 - accuracy: 0.8202 - val_loss: 0.3950 - val_accuracy: 0.8229\n",
      "Epoch 729/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3745 - accuracy: 0.8198 - val_loss: 0.3960 - val_accuracy: 0.8171\n",
      "Epoch 730/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3693 - accuracy: 0.8240 - val_loss: 0.3967 - val_accuracy: 0.8251\n",
      "Epoch 731/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3691 - accuracy: 0.8259 - val_loss: 0.3937 - val_accuracy: 0.8251\n",
      "Epoch 732/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3751 - accuracy: 0.8206 - val_loss: 0.3970 - val_accuracy: 0.8183\n",
      "Epoch 733/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3735 - accuracy: 0.8206 - val_loss: 0.3937 - val_accuracy: 0.8206\n",
      "Epoch 734/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3677 - accuracy: 0.8221 - val_loss: 0.3927 - val_accuracy: 0.8229\n",
      "Epoch 735/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3747 - accuracy: 0.8187 - val_loss: 0.3914 - val_accuracy: 0.8240\n",
      "Epoch 736/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3711 - accuracy: 0.8232 - val_loss: 0.3915 - val_accuracy: 0.8251\n",
      "Epoch 737/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3666 - accuracy: 0.8248 - val_loss: 0.3990 - val_accuracy: 0.8183\n",
      "Epoch 738/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3691 - accuracy: 0.8274 - val_loss: 0.3924 - val_accuracy: 0.8206\n",
      "Epoch 739/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3669 - accuracy: 0.8270 - val_loss: 0.3950 - val_accuracy: 0.8251\n",
      "Epoch 740/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3674 - accuracy: 0.8282 - val_loss: 0.3931 - val_accuracy: 0.8229\n",
      "Epoch 741/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3684 - accuracy: 0.8232 - val_loss: 0.3936 - val_accuracy: 0.8229\n",
      "Epoch 742/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3720 - accuracy: 0.8270 - val_loss: 0.3957 - val_accuracy: 0.8217\n",
      "Epoch 743/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3662 - accuracy: 0.8305 - val_loss: 0.3924 - val_accuracy: 0.8263\n",
      "Epoch 744/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3698 - accuracy: 0.8236 - val_loss: 0.3935 - val_accuracy: 0.8251\n",
      "Epoch 745/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3674 - accuracy: 0.8240 - val_loss: 0.3969 - val_accuracy: 0.8217\n",
      "Epoch 746/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3650 - accuracy: 0.8301 - val_loss: 0.3942 - val_accuracy: 0.8206\n",
      "Epoch 747/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3698 - accuracy: 0.8259 - val_loss: 0.3979 - val_accuracy: 0.8194\n",
      "Epoch 748/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3682 - accuracy: 0.8240 - val_loss: 0.3917 - val_accuracy: 0.8251\n",
      "Epoch 749/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3730 - accuracy: 0.8259 - val_loss: 0.3932 - val_accuracy: 0.8240\n",
      "Epoch 750/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3649 - accuracy: 0.8309 - val_loss: 0.3930 - val_accuracy: 0.8229\n",
      "Epoch 751/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3708 - accuracy: 0.8263 - val_loss: 0.3919 - val_accuracy: 0.8229\n",
      "Epoch 752/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3713 - accuracy: 0.8244 - val_loss: 0.3948 - val_accuracy: 0.8229\n",
      "Epoch 753/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3680 - accuracy: 0.8229 - val_loss: 0.3941 - val_accuracy: 0.8206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 754/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3713 - accuracy: 0.8259 - val_loss: 0.3924 - val_accuracy: 0.8274\n",
      "Epoch 755/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3669 - accuracy: 0.8244 - val_loss: 0.3938 - val_accuracy: 0.8229\n",
      "Epoch 756/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3718 - accuracy: 0.8255 - val_loss: 0.3926 - val_accuracy: 0.8229\n",
      "Epoch 757/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3657 - accuracy: 0.8282 - val_loss: 0.3932 - val_accuracy: 0.8263\n",
      "Epoch 758/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3695 - accuracy: 0.8225 - val_loss: 0.3921 - val_accuracy: 0.8217\n",
      "Epoch 759/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3713 - accuracy: 0.8175 - val_loss: 0.3937 - val_accuracy: 0.8229\n",
      "Epoch 760/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3650 - accuracy: 0.8309 - val_loss: 0.3899 - val_accuracy: 0.8263\n",
      "Epoch 761/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3746 - accuracy: 0.8267 - val_loss: 0.3917 - val_accuracy: 0.8229\n",
      "Epoch 762/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3702 - accuracy: 0.8251 - val_loss: 0.3954 - val_accuracy: 0.8217\n",
      "Epoch 763/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3710 - accuracy: 0.8316 - val_loss: 0.3941 - val_accuracy: 0.8251\n",
      "Epoch 764/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3671 - accuracy: 0.8263 - val_loss: 0.3931 - val_accuracy: 0.8263\n",
      "Epoch 765/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3709 - accuracy: 0.8267 - val_loss: 0.3952 - val_accuracy: 0.8171\n",
      "Epoch 766/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3689 - accuracy: 0.8225 - val_loss: 0.3953 - val_accuracy: 0.8183\n",
      "Epoch 767/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3656 - accuracy: 0.8290 - val_loss: 0.3987 - val_accuracy: 0.8240\n",
      "Epoch 768/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3716 - accuracy: 0.8229 - val_loss: 0.3917 - val_accuracy: 0.8263\n",
      "Epoch 769/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3694 - accuracy: 0.8255 - val_loss: 0.3944 - val_accuracy: 0.8240\n",
      "Epoch 770/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3717 - accuracy: 0.8240 - val_loss: 0.3923 - val_accuracy: 0.8194\n",
      "Epoch 771/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3767 - accuracy: 0.8255 - val_loss: 0.3915 - val_accuracy: 0.8263\n",
      "Epoch 772/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3683 - accuracy: 0.8267 - val_loss: 0.3967 - val_accuracy: 0.8240\n",
      "Epoch 773/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3691 - accuracy: 0.8290 - val_loss: 0.3908 - val_accuracy: 0.8274\n",
      "Epoch 774/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3645 - accuracy: 0.8263 - val_loss: 0.3971 - val_accuracy: 0.8194\n",
      "Epoch 775/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3736 - accuracy: 0.8194 - val_loss: 0.3912 - val_accuracy: 0.8251\n",
      "Epoch 776/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3708 - accuracy: 0.8244 - val_loss: 0.3968 - val_accuracy: 0.8229\n",
      "Epoch 777/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3668 - accuracy: 0.8240 - val_loss: 0.3963 - val_accuracy: 0.8217\n",
      "Epoch 778/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3673 - accuracy: 0.8270 - val_loss: 0.3950 - val_accuracy: 0.8206\n",
      "Epoch 779/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3676 - accuracy: 0.8229 - val_loss: 0.3937 - val_accuracy: 0.8229\n",
      "Epoch 780/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3677 - accuracy: 0.8221 - val_loss: 0.3926 - val_accuracy: 0.8297\n",
      "Epoch 781/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3695 - accuracy: 0.8309 - val_loss: 0.3930 - val_accuracy: 0.8263\n",
      "Epoch 782/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3650 - accuracy: 0.8297 - val_loss: 0.3928 - val_accuracy: 0.8194\n",
      "Epoch 783/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3679 - accuracy: 0.8286 - val_loss: 0.3945 - val_accuracy: 0.8229\n",
      "Epoch 784/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3680 - accuracy: 0.8270 - val_loss: 0.3922 - val_accuracy: 0.8263\n",
      "Epoch 785/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3660 - accuracy: 0.8259 - val_loss: 0.3904 - val_accuracy: 0.8240\n",
      "Epoch 786/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3689 - accuracy: 0.8263 - val_loss: 0.3949 - val_accuracy: 0.8194\n",
      "Epoch 787/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3661 - accuracy: 0.8267 - val_loss: 0.3937 - val_accuracy: 0.8217\n",
      "Epoch 788/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3683 - accuracy: 0.8267 - val_loss: 0.3931 - val_accuracy: 0.8297\n",
      "Epoch 789/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3717 - accuracy: 0.8236 - val_loss: 0.3918 - val_accuracy: 0.8297\n",
      "Epoch 790/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3705 - accuracy: 0.8251 - val_loss: 0.3918 - val_accuracy: 0.8240\n",
      "Epoch 791/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3686 - accuracy: 0.8278 - val_loss: 0.3990 - val_accuracy: 0.8114\n",
      "Epoch 792/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3716 - accuracy: 0.8286 - val_loss: 0.3919 - val_accuracy: 0.8206\n",
      "Epoch 793/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3719 - accuracy: 0.8240 - val_loss: 0.3944 - val_accuracy: 0.8240\n",
      "Epoch 794/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3640 - accuracy: 0.8286 - val_loss: 0.3921 - val_accuracy: 0.8251\n",
      "Epoch 795/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3663 - accuracy: 0.8255 - val_loss: 0.3913 - val_accuracy: 0.8217\n",
      "Epoch 796/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3697 - accuracy: 0.8213 - val_loss: 0.3950 - val_accuracy: 0.8183\n",
      "Epoch 797/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3663 - accuracy: 0.8274 - val_loss: 0.3918 - val_accuracy: 0.8229\n",
      "Epoch 798/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3661 - accuracy: 0.8263 - val_loss: 0.3922 - val_accuracy: 0.8251\n",
      "Epoch 799/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3666 - accuracy: 0.8240 - val_loss: 0.3924 - val_accuracy: 0.8194\n",
      "Epoch 800/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3620 - accuracy: 0.8324 - val_loss: 0.3900 - val_accuracy: 0.8274\n",
      "Epoch 801/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3649 - accuracy: 0.8263 - val_loss: 0.3963 - val_accuracy: 0.8206\n",
      "Epoch 802/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3704 - accuracy: 0.8232 - val_loss: 0.3949 - val_accuracy: 0.8194\n",
      "Epoch 803/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3717 - accuracy: 0.8255 - val_loss: 0.3928 - val_accuracy: 0.8240\n",
      "Epoch 804/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3659 - accuracy: 0.8255 - val_loss: 0.3925 - val_accuracy: 0.8206\n",
      "Epoch 805/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3718 - accuracy: 0.8244 - val_loss: 0.3926 - val_accuracy: 0.8206\n",
      "Epoch 806/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3655 - accuracy: 0.8263 - val_loss: 0.3941 - val_accuracy: 0.8229\n",
      "Epoch 807/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3678 - accuracy: 0.8274 - val_loss: 0.3923 - val_accuracy: 0.8274\n",
      "Epoch 808/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3666 - accuracy: 0.8267 - val_loss: 0.3926 - val_accuracy: 0.8217\n",
      "Epoch 809/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3628 - accuracy: 0.8297 - val_loss: 0.3950 - val_accuracy: 0.8194\n",
      "Epoch 810/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3661 - accuracy: 0.8301 - val_loss: 0.3933 - val_accuracy: 0.8274\n",
      "Epoch 811/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3695 - accuracy: 0.8240 - val_loss: 0.3966 - val_accuracy: 0.8274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 812/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3690 - accuracy: 0.8255 - val_loss: 0.3976 - val_accuracy: 0.8149\n",
      "Epoch 813/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3669 - accuracy: 0.8286 - val_loss: 0.3932 - val_accuracy: 0.8229\n",
      "Epoch 814/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3640 - accuracy: 0.8312 - val_loss: 0.3946 - val_accuracy: 0.8206\n",
      "Epoch 815/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3610 - accuracy: 0.8305 - val_loss: 0.3923 - val_accuracy: 0.8240\n",
      "Epoch 816/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3694 - accuracy: 0.8255 - val_loss: 0.3981 - val_accuracy: 0.8206\n",
      "Epoch 817/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3707 - accuracy: 0.8221 - val_loss: 0.3948 - val_accuracy: 0.8229\n",
      "Epoch 818/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3707 - accuracy: 0.8236 - val_loss: 0.3937 - val_accuracy: 0.8183\n",
      "Epoch 819/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3660 - accuracy: 0.8267 - val_loss: 0.3946 - val_accuracy: 0.8206\n",
      "Epoch 820/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3662 - accuracy: 0.8232 - val_loss: 0.3899 - val_accuracy: 0.8263\n",
      "Epoch 821/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3701 - accuracy: 0.8263 - val_loss: 0.3921 - val_accuracy: 0.8274\n",
      "Epoch 822/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3695 - accuracy: 0.8251 - val_loss: 0.3920 - val_accuracy: 0.8217\n",
      "Epoch 823/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3655 - accuracy: 0.8270 - val_loss: 0.3926 - val_accuracy: 0.8263\n",
      "Epoch 824/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3657 - accuracy: 0.8278 - val_loss: 0.3921 - val_accuracy: 0.8274\n",
      "Epoch 825/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3657 - accuracy: 0.8263 - val_loss: 0.3928 - val_accuracy: 0.8229\n",
      "Epoch 826/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3712 - accuracy: 0.8312 - val_loss: 0.4005 - val_accuracy: 0.8149\n",
      "Epoch 827/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3691 - accuracy: 0.8248 - val_loss: 0.3950 - val_accuracy: 0.8194\n",
      "Epoch 828/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3656 - accuracy: 0.8244 - val_loss: 0.4015 - val_accuracy: 0.8229\n",
      "Epoch 829/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3783 - accuracy: 0.8210 - val_loss: 0.3964 - val_accuracy: 0.8251\n",
      "Epoch 830/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3676 - accuracy: 0.8267 - val_loss: 0.3936 - val_accuracy: 0.8217\n",
      "Epoch 831/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3714 - accuracy: 0.8263 - val_loss: 0.3932 - val_accuracy: 0.8217\n",
      "Epoch 832/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3728 - accuracy: 0.8236 - val_loss: 0.3907 - val_accuracy: 0.8331\n",
      "Epoch 833/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3701 - accuracy: 0.8324 - val_loss: 0.3958 - val_accuracy: 0.8229\n",
      "Epoch 834/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3629 - accuracy: 0.8305 - val_loss: 0.3908 - val_accuracy: 0.8171\n",
      "Epoch 835/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3664 - accuracy: 0.8251 - val_loss: 0.3954 - val_accuracy: 0.8240\n",
      "Epoch 836/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3645 - accuracy: 0.8305 - val_loss: 0.3925 - val_accuracy: 0.8240\n",
      "Epoch 837/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3667 - accuracy: 0.8270 - val_loss: 0.3960 - val_accuracy: 0.8171\n",
      "Epoch 838/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3682 - accuracy: 0.8267 - val_loss: 0.3934 - val_accuracy: 0.8206\n",
      "Epoch 839/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3640 - accuracy: 0.8301 - val_loss: 0.3919 - val_accuracy: 0.8229\n",
      "Epoch 840/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3658 - accuracy: 0.8282 - val_loss: 0.3920 - val_accuracy: 0.8240\n",
      "Epoch 841/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3639 - accuracy: 0.8305 - val_loss: 0.3932 - val_accuracy: 0.8206\n",
      "Epoch 842/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3643 - accuracy: 0.8278 - val_loss: 0.3918 - val_accuracy: 0.8240\n",
      "Epoch 843/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3726 - accuracy: 0.8202 - val_loss: 0.3938 - val_accuracy: 0.8251\n",
      "Epoch 844/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3654 - accuracy: 0.8347 - val_loss: 0.3949 - val_accuracy: 0.8217\n",
      "Epoch 845/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3642 - accuracy: 0.8286 - val_loss: 0.3929 - val_accuracy: 0.8194\n",
      "Epoch 846/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3675 - accuracy: 0.8244 - val_loss: 0.3941 - val_accuracy: 0.8251\n",
      "Epoch 847/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3668 - accuracy: 0.8290 - val_loss: 0.3931 - val_accuracy: 0.8274\n",
      "Epoch 848/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3701 - accuracy: 0.8229 - val_loss: 0.3912 - val_accuracy: 0.8286\n",
      "Epoch 849/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3658 - accuracy: 0.8240 - val_loss: 0.3930 - val_accuracy: 0.8240\n",
      "Epoch 850/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3656 - accuracy: 0.8301 - val_loss: 0.3907 - val_accuracy: 0.8251\n",
      "Epoch 851/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3612 - accuracy: 0.8320 - val_loss: 0.3921 - val_accuracy: 0.8240\n",
      "Epoch 852/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3664 - accuracy: 0.8251 - val_loss: 0.3940 - val_accuracy: 0.8206\n",
      "Epoch 853/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3672 - accuracy: 0.8278 - val_loss: 0.3922 - val_accuracy: 0.8217\n",
      "Epoch 854/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3654 - accuracy: 0.8244 - val_loss: 0.3898 - val_accuracy: 0.8240\n",
      "Epoch 855/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3663 - accuracy: 0.8320 - val_loss: 0.3936 - val_accuracy: 0.8240\n",
      "Epoch 856/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3693 - accuracy: 0.8263 - val_loss: 0.3925 - val_accuracy: 0.8309\n",
      "Epoch 857/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3625 - accuracy: 0.8293 - val_loss: 0.3966 - val_accuracy: 0.8206\n",
      "Epoch 858/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3693 - accuracy: 0.8297 - val_loss: 0.3915 - val_accuracy: 0.8194\n",
      "Epoch 859/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3652 - accuracy: 0.8286 - val_loss: 0.3906 - val_accuracy: 0.8263\n",
      "Epoch 860/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3626 - accuracy: 0.8305 - val_loss: 0.3943 - val_accuracy: 0.8251\n",
      "Epoch 861/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3652 - accuracy: 0.8255 - val_loss: 0.3914 - val_accuracy: 0.8263\n",
      "Epoch 862/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3680 - accuracy: 0.8282 - val_loss: 0.3987 - val_accuracy: 0.8183\n",
      "Epoch 863/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3654 - accuracy: 0.8290 - val_loss: 0.3918 - val_accuracy: 0.8263\n",
      "Epoch 864/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3672 - accuracy: 0.8251 - val_loss: 0.3954 - val_accuracy: 0.8274\n",
      "Epoch 865/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3687 - accuracy: 0.8305 - val_loss: 0.3962 - val_accuracy: 0.8183\n",
      "Epoch 866/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3637 - accuracy: 0.8290 - val_loss: 0.3931 - val_accuracy: 0.8229\n",
      "Epoch 867/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3639 - accuracy: 0.8259 - val_loss: 0.3958 - val_accuracy: 0.8171\n",
      "Epoch 868/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3629 - accuracy: 0.8270 - val_loss: 0.3926 - val_accuracy: 0.8274\n",
      "Epoch 869/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3578 - accuracy: 0.8290 - val_loss: 0.3923 - val_accuracy: 0.8274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 870/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3618 - accuracy: 0.8274 - val_loss: 0.3907 - val_accuracy: 0.8217\n",
      "Epoch 871/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3683 - accuracy: 0.8309 - val_loss: 0.3904 - val_accuracy: 0.8286\n",
      "Epoch 872/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3638 - accuracy: 0.8290 - val_loss: 0.3954 - val_accuracy: 0.8240\n",
      "Epoch 873/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3644 - accuracy: 0.8251 - val_loss: 0.3957 - val_accuracy: 0.8251\n",
      "Epoch 874/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3601 - accuracy: 0.8301 - val_loss: 0.3943 - val_accuracy: 0.8240\n",
      "Epoch 875/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3608 - accuracy: 0.8335 - val_loss: 0.3931 - val_accuracy: 0.8229\n",
      "Epoch 876/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3628 - accuracy: 0.8301 - val_loss: 0.3930 - val_accuracy: 0.8309\n",
      "Epoch 877/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3629 - accuracy: 0.8350 - val_loss: 0.3904 - val_accuracy: 0.8274\n",
      "Epoch 878/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3620 - accuracy: 0.8320 - val_loss: 0.3930 - val_accuracy: 0.8217\n",
      "Epoch 879/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3605 - accuracy: 0.8301 - val_loss: 0.3899 - val_accuracy: 0.8240\n",
      "Epoch 880/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3646 - accuracy: 0.8278 - val_loss: 0.3921 - val_accuracy: 0.8217\n",
      "Epoch 881/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3650 - accuracy: 0.8297 - val_loss: 0.3904 - val_accuracy: 0.8274\n",
      "Epoch 882/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3609 - accuracy: 0.8331 - val_loss: 0.3912 - val_accuracy: 0.8297\n",
      "Epoch 883/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3610 - accuracy: 0.8316 - val_loss: 0.3948 - val_accuracy: 0.8194\n",
      "Epoch 884/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3635 - accuracy: 0.8263 - val_loss: 0.3919 - val_accuracy: 0.8240\n",
      "Epoch 885/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3644 - accuracy: 0.8320 - val_loss: 0.3902 - val_accuracy: 0.8263\n",
      "Epoch 886/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3618 - accuracy: 0.8274 - val_loss: 0.3906 - val_accuracy: 0.8251\n",
      "Epoch 887/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3650 - accuracy: 0.8229 - val_loss: 0.3914 - val_accuracy: 0.8194\n",
      "Epoch 888/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3670 - accuracy: 0.8328 - val_loss: 0.3894 - val_accuracy: 0.8206\n",
      "Epoch 889/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3631 - accuracy: 0.8309 - val_loss: 0.3898 - val_accuracy: 0.8229\n",
      "Epoch 890/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3620 - accuracy: 0.8305 - val_loss: 0.3891 - val_accuracy: 0.8251\n",
      "Epoch 891/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3670 - accuracy: 0.8305 - val_loss: 0.3897 - val_accuracy: 0.8251\n",
      "Epoch 892/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3634 - accuracy: 0.8263 - val_loss: 0.3898 - val_accuracy: 0.8309\n",
      "Epoch 893/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3650 - accuracy: 0.8255 - val_loss: 0.3898 - val_accuracy: 0.8251\n",
      "Epoch 894/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3645 - accuracy: 0.8305 - val_loss: 0.3904 - val_accuracy: 0.8217\n",
      "Epoch 895/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3666 - accuracy: 0.8270 - val_loss: 0.3944 - val_accuracy: 0.8240\n",
      "Epoch 896/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3565 - accuracy: 0.8316 - val_loss: 0.3896 - val_accuracy: 0.8240\n",
      "Epoch 897/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3627 - accuracy: 0.8320 - val_loss: 0.3965 - val_accuracy: 0.8229\n",
      "Epoch 898/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3643 - accuracy: 0.8328 - val_loss: 0.3905 - val_accuracy: 0.8320\n",
      "Epoch 899/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3599 - accuracy: 0.8297 - val_loss: 0.3943 - val_accuracy: 0.8229\n",
      "Epoch 900/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3636 - accuracy: 0.8293 - val_loss: 0.3894 - val_accuracy: 0.8263\n",
      "Epoch 901/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3625 - accuracy: 0.8255 - val_loss: 0.3890 - val_accuracy: 0.8240\n",
      "Epoch 902/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3637 - accuracy: 0.8305 - val_loss: 0.3899 - val_accuracy: 0.8217\n",
      "Epoch 903/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3620 - accuracy: 0.8331 - val_loss: 0.3939 - val_accuracy: 0.8183\n",
      "Epoch 904/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3599 - accuracy: 0.8335 - val_loss: 0.3895 - val_accuracy: 0.8274\n",
      "Epoch 905/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3618 - accuracy: 0.8270 - val_loss: 0.3936 - val_accuracy: 0.8206\n",
      "Epoch 906/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3612 - accuracy: 0.8328 - val_loss: 0.3888 - val_accuracy: 0.8240\n",
      "Epoch 907/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3689 - accuracy: 0.8290 - val_loss: 0.3907 - val_accuracy: 0.8229\n",
      "Epoch 908/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3692 - accuracy: 0.8278 - val_loss: 0.3898 - val_accuracy: 0.8217\n",
      "Epoch 909/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3581 - accuracy: 0.8377 - val_loss: 0.3918 - val_accuracy: 0.8229\n",
      "Epoch 910/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3592 - accuracy: 0.8331 - val_loss: 0.3898 - val_accuracy: 0.8263\n",
      "Epoch 911/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3592 - accuracy: 0.8305 - val_loss: 0.3899 - val_accuracy: 0.8263\n",
      "Epoch 912/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3604 - accuracy: 0.8293 - val_loss: 0.3923 - val_accuracy: 0.8229\n",
      "Epoch 913/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3590 - accuracy: 0.8301 - val_loss: 0.3946 - val_accuracy: 0.8217\n",
      "Epoch 914/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3674 - accuracy: 0.8309 - val_loss: 0.3925 - val_accuracy: 0.8171\n",
      "Epoch 915/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3650 - accuracy: 0.8305 - val_loss: 0.3884 - val_accuracy: 0.8309\n",
      "Epoch 916/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3637 - accuracy: 0.8270 - val_loss: 0.3969 - val_accuracy: 0.8206\n",
      "Epoch 917/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3644 - accuracy: 0.8301 - val_loss: 0.3889 - val_accuracy: 0.8320\n",
      "Epoch 918/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3659 - accuracy: 0.8339 - val_loss: 0.3985 - val_accuracy: 0.8126\n",
      "Epoch 919/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3673 - accuracy: 0.8293 - val_loss: 0.3926 - val_accuracy: 0.8206\n",
      "Epoch 920/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3611 - accuracy: 0.8290 - val_loss: 0.3884 - val_accuracy: 0.8297\n",
      "Epoch 921/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3619 - accuracy: 0.8335 - val_loss: 0.3938 - val_accuracy: 0.8206\n",
      "Epoch 922/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3649 - accuracy: 0.8297 - val_loss: 0.3882 - val_accuracy: 0.8286\n",
      "Epoch 923/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3585 - accuracy: 0.8370 - val_loss: 0.3897 - val_accuracy: 0.8217\n",
      "Epoch 924/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3664 - accuracy: 0.8255 - val_loss: 0.3887 - val_accuracy: 0.8229\n",
      "Epoch 925/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3623 - accuracy: 0.8347 - val_loss: 0.3971 - val_accuracy: 0.8240\n",
      "Epoch 926/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3653 - accuracy: 0.8305 - val_loss: 0.3890 - val_accuracy: 0.8251\n",
      "Epoch 927/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3615 - accuracy: 0.8343 - val_loss: 0.3881 - val_accuracy: 0.8309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 928/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3612 - accuracy: 0.8343 - val_loss: 0.3915 - val_accuracy: 0.8251\n",
      "Epoch 929/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3624 - accuracy: 0.8350 - val_loss: 0.3885 - val_accuracy: 0.8251\n",
      "Epoch 930/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3653 - accuracy: 0.8274 - val_loss: 0.3885 - val_accuracy: 0.8274\n",
      "Epoch 931/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3617 - accuracy: 0.8290 - val_loss: 0.4015 - val_accuracy: 0.8206\n",
      "Epoch 932/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3778 - accuracy: 0.8229 - val_loss: 0.3884 - val_accuracy: 0.8251\n",
      "Epoch 933/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3737 - accuracy: 0.8240 - val_loss: 0.3871 - val_accuracy: 0.8240\n",
      "Epoch 934/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3720 - accuracy: 0.8309 - val_loss: 0.3955 - val_accuracy: 0.8229\n",
      "Epoch 935/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3656 - accuracy: 0.8297 - val_loss: 0.3893 - val_accuracy: 0.8274\n",
      "Epoch 936/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3622 - accuracy: 0.8290 - val_loss: 0.3970 - val_accuracy: 0.8217\n",
      "Epoch 937/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3652 - accuracy: 0.8328 - val_loss: 0.3905 - val_accuracy: 0.8240\n",
      "Epoch 938/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3592 - accuracy: 0.8358 - val_loss: 0.3983 - val_accuracy: 0.8217\n",
      "Epoch 939/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3583 - accuracy: 0.8267 - val_loss: 0.3895 - val_accuracy: 0.8194\n",
      "Epoch 940/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3591 - accuracy: 0.8335 - val_loss: 0.3910 - val_accuracy: 0.8217\n",
      "Epoch 941/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3559 - accuracy: 0.8312 - val_loss: 0.3883 - val_accuracy: 0.8206\n",
      "Epoch 942/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3589 - accuracy: 0.8335 - val_loss: 0.3904 - val_accuracy: 0.8229\n",
      "Epoch 943/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3584 - accuracy: 0.8370 - val_loss: 0.3908 - val_accuracy: 0.8217\n",
      "Epoch 944/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3633 - accuracy: 0.8267 - val_loss: 0.3942 - val_accuracy: 0.8171\n",
      "Epoch 945/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3557 - accuracy: 0.8366 - val_loss: 0.3934 - val_accuracy: 0.8183\n",
      "Epoch 946/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3662 - accuracy: 0.8324 - val_loss: 0.3891 - val_accuracy: 0.8263\n",
      "Epoch 947/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3669 - accuracy: 0.8263 - val_loss: 0.3897 - val_accuracy: 0.8217\n",
      "Epoch 948/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3592 - accuracy: 0.8335 - val_loss: 0.3906 - val_accuracy: 0.8251\n",
      "Epoch 949/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3603 - accuracy: 0.8358 - val_loss: 0.3893 - val_accuracy: 0.8286\n",
      "Epoch 950/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3561 - accuracy: 0.8339 - val_loss: 0.3915 - val_accuracy: 0.8240\n",
      "Epoch 951/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3558 - accuracy: 0.8366 - val_loss: 0.3895 - val_accuracy: 0.8229\n",
      "Epoch 952/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3626 - accuracy: 0.8267 - val_loss: 0.3933 - val_accuracy: 0.8274\n",
      "Epoch 953/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3619 - accuracy: 0.8278 - val_loss: 0.3903 - val_accuracy: 0.8240\n",
      "Epoch 954/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3620 - accuracy: 0.8362 - val_loss: 0.3906 - val_accuracy: 0.8251\n",
      "Epoch 955/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3552 - accuracy: 0.8339 - val_loss: 0.3948 - val_accuracy: 0.8251\n",
      "Epoch 956/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3613 - accuracy: 0.8354 - val_loss: 0.3901 - val_accuracy: 0.8263\n",
      "Epoch 957/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3576 - accuracy: 0.8331 - val_loss: 0.3909 - val_accuracy: 0.8240\n",
      "Epoch 958/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3595 - accuracy: 0.8320 - val_loss: 0.3927 - val_accuracy: 0.8194\n",
      "Epoch 959/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3648 - accuracy: 0.8312 - val_loss: 0.3876 - val_accuracy: 0.8274\n",
      "Epoch 960/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3601 - accuracy: 0.8331 - val_loss: 0.3885 - val_accuracy: 0.8274\n",
      "Epoch 961/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3586 - accuracy: 0.8324 - val_loss: 0.3903 - val_accuracy: 0.8217\n",
      "Epoch 962/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3602 - accuracy: 0.8328 - val_loss: 0.3895 - val_accuracy: 0.8217\n",
      "Epoch 963/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3647 - accuracy: 0.8282 - val_loss: 0.3901 - val_accuracy: 0.8240\n",
      "Epoch 964/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3620 - accuracy: 0.8293 - val_loss: 0.3900 - val_accuracy: 0.8354\n",
      "Epoch 965/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3637 - accuracy: 0.8282 - val_loss: 0.3969 - val_accuracy: 0.8229\n",
      "Epoch 966/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3621 - accuracy: 0.8335 - val_loss: 0.3924 - val_accuracy: 0.8263\n",
      "Epoch 967/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3656 - accuracy: 0.8324 - val_loss: 0.3921 - val_accuracy: 0.8263\n",
      "Epoch 968/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3610 - accuracy: 0.8286 - val_loss: 0.3910 - val_accuracy: 0.8240\n",
      "Epoch 969/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3693 - accuracy: 0.8270 - val_loss: 0.3908 - val_accuracy: 0.8251\n",
      "Epoch 970/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3649 - accuracy: 0.8297 - val_loss: 0.3953 - val_accuracy: 0.8229\n",
      "Epoch 971/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3584 - accuracy: 0.8312 - val_loss: 0.3886 - val_accuracy: 0.8251\n",
      "Epoch 972/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3592 - accuracy: 0.8305 - val_loss: 0.3966 - val_accuracy: 0.8149\n",
      "Epoch 973/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3603 - accuracy: 0.8309 - val_loss: 0.3884 - val_accuracy: 0.8309\n",
      "Epoch 974/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3585 - accuracy: 0.8331 - val_loss: 0.3940 - val_accuracy: 0.8206\n",
      "Epoch 975/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3568 - accuracy: 0.8389 - val_loss: 0.3905 - val_accuracy: 0.8297\n",
      "Epoch 976/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3540 - accuracy: 0.8343 - val_loss: 0.3885 - val_accuracy: 0.8309\n",
      "Epoch 977/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3589 - accuracy: 0.8331 - val_loss: 0.3941 - val_accuracy: 0.8251\n",
      "Epoch 978/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3603 - accuracy: 0.8331 - val_loss: 0.3874 - val_accuracy: 0.8343\n",
      "Epoch 979/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3692 - accuracy: 0.8240 - val_loss: 0.3909 - val_accuracy: 0.8240\n",
      "Epoch 980/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3552 - accuracy: 0.8316 - val_loss: 0.3900 - val_accuracy: 0.8206\n",
      "Epoch 981/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3586 - accuracy: 0.8347 - val_loss: 0.3894 - val_accuracy: 0.8263\n",
      "Epoch 982/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3571 - accuracy: 0.8328 - val_loss: 0.3941 - val_accuracy: 0.8206\n",
      "Epoch 983/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3642 - accuracy: 0.8290 - val_loss: 0.3909 - val_accuracy: 0.8229\n",
      "Epoch 984/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3565 - accuracy: 0.8350 - val_loss: 0.3977 - val_accuracy: 0.8171\n",
      "Epoch 985/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3643 - accuracy: 0.8309 - val_loss: 0.3916 - val_accuracy: 0.8274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 986/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3571 - accuracy: 0.8362 - val_loss: 0.3889 - val_accuracy: 0.8297\n",
      "Epoch 987/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3595 - accuracy: 0.8301 - val_loss: 0.3954 - val_accuracy: 0.8217\n",
      "Epoch 988/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3591 - accuracy: 0.8324 - val_loss: 0.3881 - val_accuracy: 0.8251\n",
      "Epoch 989/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3618 - accuracy: 0.8324 - val_loss: 0.3963 - val_accuracy: 0.8263\n",
      "Epoch 990/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3640 - accuracy: 0.8278 - val_loss: 0.3884 - val_accuracy: 0.8274\n",
      "Epoch 991/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3596 - accuracy: 0.8362 - val_loss: 0.3870 - val_accuracy: 0.8320\n",
      "Epoch 992/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3581 - accuracy: 0.8358 - val_loss: 0.3891 - val_accuracy: 0.8286\n",
      "Epoch 993/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3592 - accuracy: 0.8339 - val_loss: 0.3872 - val_accuracy: 0.8331\n",
      "Epoch 994/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3558 - accuracy: 0.8389 - val_loss: 0.3889 - val_accuracy: 0.8206\n",
      "Epoch 995/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3591 - accuracy: 0.8324 - val_loss: 0.3896 - val_accuracy: 0.8206\n",
      "Epoch 996/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3582 - accuracy: 0.8328 - val_loss: 0.3881 - val_accuracy: 0.8263\n",
      "Epoch 997/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3578 - accuracy: 0.8339 - val_loss: 0.3895 - val_accuracy: 0.8274\n",
      "Epoch 998/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3585 - accuracy: 0.8347 - val_loss: 0.3887 - val_accuracy: 0.8274\n",
      "Epoch 999/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3603 - accuracy: 0.8290 - val_loss: 0.3905 - val_accuracy: 0.8229\n",
      "Epoch 1000/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3601 - accuracy: 0.8366 - val_loss: 0.3899 - val_accuracy: 0.8240\n"
     ]
    }
   ],
   "source": [
    "history2 = seq_model.fit(X_train,\n",
    "                        y_train,\n",
    "                        epochs=1000,\n",
    "                        validation_data=(X_val, y_val),\n",
    "                        batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.8381\n",
      "Validation Accuracy:  0.8240\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = seq_model.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = seq_model.evaluate(X_val, y_val, verbose=False)\n",
    "print(\"Validation Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, acc, 'b', label='Training acc')\n",
    "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_history(history2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential Model\n",
    "y_pred_seq_model = seq_model.predict(X_val)\n",
    "y_pred_seq = np.round(y_pred_seq_model).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "440 77\n",
      "77 281\n",
      "correct: 721 incorrect: 154\n"
     ]
    }
   ],
   "source": [
    "a,b,c,d = confusion_matrix(y_val, y_pred_seq).ravel()\n",
    "print(a, b)\n",
    "print(c, d)\n",
    "print(\"correct:\",str(a+d), \"incorrect:\",str(b+c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission using Logistic Regression (logReg_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df[\"pred\"] = logReg_model.predict(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df[\"pred\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv(\"output/submission_lr_2.csv\", header = False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission using Naives Bayes Classifier (naiveBayes_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df[\"pred\"] = naiveBayes_model.predict(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv(\"output/submission_naive.csv\", header = False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission using ExtraTrees (et_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_best = test_df[[\"Age\", \"Gender\", \"T_Bil\", \"ALP\" ,\"ALT_GPT\" ,\"TP\", \"Alb\", \"AG_ratio\" ,\"B_Bil\" ,\"AST_ALT_ratio\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df[\"pred\"] = et_clf.predict(test_df_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv(\"output/NEW_best_extra_trees.csv\", header = False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission using Random Forest (rf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df[\"pred\"] = rf_model.predict(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv(\"output/submission_random.csv\", header = False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_seq_model = seq_model.predict(test_df)\n",
    "# y_pred_seq = np.round(y_pred_seq_model).astype(int)\n",
    "submission_df[\"pred\"] = y_pred_seq_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv(\"output/Seq_0-1.csv\", header = False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGLB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_light = gbm.predict(test_df)\n",
    "# y_pred_light_model = np.round(y_pred_light).astype(int)\n",
    "submission_df[\"pred\"] = y_pred_light"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv(\"output/l2.csv\", header = False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
